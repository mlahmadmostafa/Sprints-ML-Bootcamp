{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "chunk_generator = pd.read_csv(\"en-fr.csv\", chunksize=20000)\n",
    "df = next(chunk_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)\n",
    "df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 64  # Maximum length of padded sequences\n",
    "\n",
    "class Preprocessor:\n",
    "    def __init__(self):\n",
    "        self.stemmer = PorterStemmer() # stemmer\n",
    "\n",
    "    def apply_preprocessing(self, text, sos = True, eos = True):\n",
    "        text = text.lower() # convert to lowercase\n",
    "        text = self.remove_characters(text)\n",
    "        text = remove_stopwords(text) # remove stopwords\n",
    "        # text = self.stemming(text)\n",
    "        text = self.truncate(text)\n",
    "        text = self.add_os_tokens(text, sos, eos)\n",
    "        return text\n",
    "    \n",
    "    def stemming(self, text):\n",
    "        return \" \".join([self.stemmer.stem(word) for word in text.split()])\n",
    "\n",
    "    def remove_characters(self, text):\n",
    "        emojis = r\"[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F1E0-\\U0001F1FF]+\"  # regex for emojis\n",
    "        html = r'http\\S+|www\\S+' # regex for urls\n",
    "        mentions = r\"@\\w+\" # regex for mentions\n",
    "        hashtags = r\"#\" # regex for hashtags\n",
    "        text = re.sub(f'{emojis}|{html}|{mentions}|{hashtags}','',text) # remove emojis, urls, mentions and hashtags\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
    "        return text\n",
    "\n",
    "\n",
    "    def truncate(self, text):\n",
    "        if len(text.split())>MAX_SEQUENCE_LENGTH:\n",
    "            text = text[:MAX_SEQUENCE_LENGTH-2]\n",
    "        return text\n",
    "\n",
    "    def add_os_tokens(self, text, sos = True, eos = True): # os is of sentence\n",
    "        if sos:\n",
    "            text = '<sos> ' + text\n",
    "        if eos:\n",
    "            text += ' <eos>'\n",
    "        return text\n",
    "    \n",
    "preprocessor = Preprocessor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "en",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "fr_input",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "fr_output",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "2c001ca4-8287-4a36-9512-40dd7e8d7eaa",
       "rows": [
        [
         "0",
         "<sos> changing lives changing society works technology drives change home concepts teachers search overview credits hhcc web reference feedback virtual museum canada home page <eos>",
         "<sos> il transformé notre vie il transformé la société son fonctionnement la technologie moteur du changement accueil concepts enseignants recherche aperçu collaborateurs web hhcc ressources commentaires musée virtuel du canada",
         "il transformé notre vie il transformé la société son fonctionnement la technologie moteur du changement accueil concepts enseignants recherche aperçu collaborateurs web hhcc ressources commentaires musée virtuel du canada <eos>"
        ],
        [
         "1",
         "<sos> site map <eos>",
         "<sos> plan du site",
         "plan du site <eos>"
        ],
        [
         "2",
         "<sos> feedback <eos>",
         "<sos> rétroaction",
         "rétroaction <eos>"
        ],
        [
         "3",
         "<sos> credits <eos>",
         "<sos> crédits",
         "crédits <eos>"
        ],
        [
         "4",
         "<sos> français <eos>",
         "<sos> english",
         "english <eos>"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>fr_input</th>\n",
       "      <th>fr_output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;sos&gt; changing lives changing society works te...</td>\n",
       "      <td>&lt;sos&gt; il transformé notre vie il transformé la...</td>\n",
       "      <td>il transformé notre vie il transformé la socié...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;sos&gt; site map &lt;eos&gt;</td>\n",
       "      <td>&lt;sos&gt; plan du site</td>\n",
       "      <td>plan du site &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;sos&gt; feedback &lt;eos&gt;</td>\n",
       "      <td>&lt;sos&gt; rétroaction</td>\n",
       "      <td>rétroaction &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;sos&gt; credits &lt;eos&gt;</td>\n",
       "      <td>&lt;sos&gt; crédits</td>\n",
       "      <td>crédits &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;sos&gt; français &lt;eos&gt;</td>\n",
       "      <td>&lt;sos&gt; english</td>\n",
       "      <td>english &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  en  \\\n",
       "0  <sos> changing lives changing society works te...   \n",
       "1                               <sos> site map <eos>   \n",
       "2                               <sos> feedback <eos>   \n",
       "3                                <sos> credits <eos>   \n",
       "4                               <sos> français <eos>   \n",
       "\n",
       "                                            fr_input  \\\n",
       "0  <sos> il transformé notre vie il transformé la...   \n",
       "1                                 <sos> plan du site   \n",
       "2                                  <sos> rétroaction   \n",
       "3                                      <sos> crédits   \n",
       "4                                      <sos> english   \n",
       "\n",
       "                                           fr_output  \n",
       "0  il transformé notre vie il transformé la socié...  \n",
       "1                                 plan du site <eos>  \n",
       "2                                  rétroaction <eos>  \n",
       "3                                      crédits <eos>  \n",
       "4                                      english <eos>  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['en'] = df['en'].apply(preprocessor.apply_preprocessing)\n",
    "\n",
    "df['fr_input'] = df['fr'].apply(lambda x: preprocessor.apply_preprocessing(x, sos=True, eos=False))\n",
    "df['fr_output'] = df['fr'].apply(lambda x: preprocessor.apply_preprocessing(x, sos=False, eos=True))\n",
    "\n",
    "df.drop('fr', axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "MAX_SEQUENCE_LENGTH = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, models, decoders, trainers\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "trainer_en = trainers.BpeTrainer(vocab_size=VOCAB_SIZE, special_tokens=[\"<pad>\", \"<sos>\", \"<eos>\", \"<unk>\"])\n",
    "trainer_fr = trainers.BpeTrainer(vocab_size=VOCAB_SIZE, special_tokens=[\"<pad>\", \"<sos>\", \"<eos>\", \"<unk>\"])\n",
    "    \n",
    "tokenizer_en = Tokenizer(models.BPE(vocab_size=VOCAB_SIZE, unk_token=\"<unk>\"))\n",
    "tokenizer_fr = Tokenizer(models.BPE(vocab_size=VOCAB_SIZE, unk_token=\"<unk>\"))\n",
    "\n",
    "\n",
    "# Add pre-tokenizers (e.g., split on whitespace)\n",
    "tokenizer_en.pre_tokenizer = Whitespace()\n",
    "tokenizer_fr.pre_tokenizer = Whitespace()\n",
    "\n",
    "# Add decoders (e.g., convert tokens back to strings)\n",
    "tokenizer_en.decoder = decoders.BPEDecoder()\n",
    "tokenizer_fr.decoder = decoders.BPEDecoder()\n",
    "\n",
    "tokenizer_en.train_from_iterator(df['en'], trainer_en)\n",
    "tokenizer_fr.train_from_iterator(df['fr_input'], trainer_fr)\n",
    "\n",
    "# Configure padding\n",
    "tokenizer_en.enable_padding(\n",
    "    pad_id=tokenizer_en.token_to_id(\"<pad>\"),  # ID of the <pad> token\n",
    "    pad_token=\"<pad>\",                      # The padding token\n",
    "    length=MAX_SEQUENCE_LENGTH,                              # Pad sequences to this length (optional)\n",
    "    direction=\"left\"                       # Pad on the right (default)\n",
    ")\n",
    "\n",
    "tokenizer_fr.enable_padding(\n",
    "    pad_id=tokenizer_fr.token_to_id(\"<pad>\"),  # ID of the <pad> token\n",
    "    pad_token=\"<pad>\",                      # The padding token\n",
    "    length=MAX_SEQUENCE_LENGTH,                              # Pad sequences to this length (optional)\n",
    "    direction=\"right\"                       # Pad on the right (default)\n",
    ")\n",
    "# Enable truncation\n",
    "tokenizer_en.enable_truncation(max_length=MAX_SEQUENCE_LENGTH, direction=\"left\")  # Set desired max length\n",
    "tokenizer_fr.enable_truncation(max_length=MAX_SEQUENCE_LENGTH, direction=\"right\")  # Set desired max length\n",
    "\n",
    "tokenizer_en.save(\"tokenizer_en.json\")\n",
    "tokenizer_fr.save(\"tokenizer_fr.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_en.get_vocab()['<pad>'], tokenizer_fr.get_vocab()['<pad>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Pre-trained GloVe Embeddings\n",
    "embedding_index = {} # Dictionary to store word embeddings\n",
    "\n",
    "with open('glove.6B.100d.txt', encoding='utf8') as glove_file:\n",
    "    for line in glove_file:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype='float32')\n",
    "        embedding_index[word] = vector\n",
    "        \n",
    "# Initialize embedding matrix\n",
    "num_words = tokenizer_en.get_vocab_size()\n",
    "embedding_matrix_en = np.zeros((num_words, EMBEDDING_DIM))\n",
    "\n",
    "# Populate embedding matrix\n",
    "for word, index in tokenizer_en.get_vocab().items():\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix_en[index] = embedding_vector\n",
    "\n",
    "# Initialize embedding matrix\n",
    "num_words = tokenizer_fr.get_vocab_size()\n",
    "embedding_matrix_fr = np.zeros((num_words, EMBEDDING_DIM))\n",
    "\n",
    "# Populate embedding matrix\n",
    "for word, index in tokenizer_en.get_vocab().items():\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix_fr[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "class Seq2SeqModel:\n",
    "    def __init__(self, input_vocab_size, output_vocab_size, embedding_dim, latent_dim, max_sequence_length):\n",
    "        self.input_vocab_size = input_vocab_size # maximum number of unique words in input\n",
    "        self.output_vocab_size = output_vocab_size # maximum number of unique words in output\n",
    "        self.embedding_dim = embedding_dim # dimension of word embeddings\n",
    "        self.latent_dim = latent_dim # dimension of latent space\n",
    "        self.max_sequence_length = max_sequence_length # maximum length of input and output sequences\n",
    "\n",
    "        # Build models\n",
    "        self.encoder = self.build_encoder()\n",
    "        self.decoder_training = self.build_decoder_training()\n",
    "        self.model = self.build_model()\n",
    "        \n",
    "    def build_encoder(self):\n",
    "        # Define the encoder\n",
    "        encoder_inputs = Input(shape=(None,), name=\"encoder_input\") # input sequence\n",
    "        encoder_embedding = Embedding(input_dim=self.input_vocab_size, # input vocabulary\n",
    "                                      output_dim=self.embedding_dim,  # dimension of word embeddings\n",
    "                                      mask_zero=True # Ignore Padding Tokens\n",
    "                                    )(encoder_inputs) # mask padding\n",
    "        encoder_outputs, state_h, state_c= LSTM(self.latent_dim, \n",
    "                                                dtype='float32',\n",
    "                                                return_state=True, # To return states\n",
    "                                                name=\"encoder_lstm\")(encoder_embedding) # LSTM cells\n",
    "\n",
    "        encoder_model = Model(encoder_inputs, [ state_h, state_c], name=\"encoder\") # Model for encoder\n",
    "        return encoder_model\n",
    "    \n",
    "    def build_decoder_training(self):\n",
    "        \n",
    "        decoder_inputs = Input(shape=(None,), name=\"decoder_input\") # input sequence from french\n",
    "        encoder_state_h = Input(shape=(self.latent_dim,), name=\"decoder_state_h\") # hidden long term memory state from encoder\n",
    "        encoder_state_c = Input(shape=(self.latent_dim,), name=\"decoder_state_c\") # hidden short term memor state from encoder\n",
    "\n",
    "        decoder_embedding = Embedding(input_dim=self.output_vocab_size, output_dim=self.embedding_dim, mask_zero=True)(decoder_inputs) # word embeddings\n",
    "        \n",
    "        # mask_zero=True Ignore Padding Tokens\n",
    "        decoder_outputs= LSTM(self.latent_dim, return_sequences=True, name=\"decoder_lstm\", dtype='float32',)\\\n",
    "            (decoder_embedding, initial_state=[encoder_state_h, encoder_state_c]) # LSTM cells\n",
    "\n",
    "        decoder_dense = Dense(self.output_vocab_size, activation='softmax')(decoder_outputs) # output layer\n",
    "\n",
    "        decoder_model = Model([decoder_inputs, encoder_state_h, encoder_state_c], \n",
    "                              decoder_dense, name=\"decoder\") # Model for decoder\n",
    "        return decoder_model\n",
    "    \n",
    "    def build_decoder_inference(self):\n",
    "        decoder_inputs = Input(shape=(None,), name=\"decoder_input\") # input sequence from french\n",
    "        decoder_state_input_h = Input(shape=(self.latent_dim,), name=\"decoder_state_h\") # previous hidden long term memory state\n",
    "        decoder_state_input_c = Input(shape=(self.latent_dim,), name=\"decoder_state_c\") # previous hidden short term memory state\n",
    "        \n",
    "        decoder_embedding = Embedding(input_dim=self.output_vocab_size, output_dim=self.embedding_dim, \n",
    "                                      mask_zero=True, # Ignore Padding Tokens\n",
    "                                      weights=[embedding_matrix_fr]\n",
    "                                    )(decoder_inputs) # word embeddings\n",
    "        \n",
    "        decoder_outputs, state_h, state_c = LSTM(self.latent_dim, return_sequences=True, return_state=True, name=\"decoder_lstm\")\\\n",
    "            (decoder_embedding, initial_state=[decoder_state_input_h, decoder_state_input_c]) # LSTM cells\n",
    "\n",
    "        decoder_dense = Dense(self.output_vocab_size, activation='softmax')(decoder_outputs) # output layer\n",
    "\n",
    "        decoder_model = Model([decoder_inputs, decoder_state_input_h, decoder_state_input_c], \n",
    "                              [decoder_dense, state_h, state_c], \n",
    "                              name=\"decoder\") # Model for decoder\n",
    "        return decoder_model\n",
    "    \n",
    "    def build_model(self):\n",
    "        \n",
    "        encoder_inputs = self.encoder.input  # Extract encoder input\n",
    "        state_h, state_c = self.encoder.output  # Extract encoder states\n",
    "        \n",
    "        decoder_inputs = self.decoder_training.input[0]  # Extract decoder input\n",
    "        decoder_outputs = self.decoder_training([decoder_inputs, state_h, state_c])  # Pass encoder states to decoder\n",
    "\n",
    "        model = Model([encoder_inputs, decoder_inputs], decoder_outputs, name=\"seq2seq_model\") # Create model\n",
    "        return model\n",
    "        \n",
    "    def compile(self, optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy']):\n",
    "        self.model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "\n",
    "    def infer(self, input_sequence, start_token_id, end_token_id, preprocess, tokenizer_en, tokenizer_fr, max_length=10):\n",
    "        # Preprocess the input\n",
    "        input_sequence = preprocess(input_sequence)\n",
    "        \n",
    "        # Tokenize the input sequence\n",
    "        input_sequence = tokenizer_en.encode(input_sequence).ids\n",
    "        input_sequence = np.array(input_sequence).reshape(1, -1)  # Reshape for model input\n",
    "        \n",
    "        # Build the inference decoder if not already built\n",
    "        if not hasattr(self, 'decoder_inference'):\n",
    "            self.decoder_inference = self.build_decoder_inference()\n",
    "        \n",
    "        # Encode the input sequence\n",
    "        states_value = self.encoder.predict(input_sequence)  # Get encoder states\n",
    "        \n",
    "        # Initialize the target sequence with the start token ID (must be integer, not string)\n",
    "        target_seq = np.array([[start_token_id]])  # Start with the start token ID\n",
    "        \n",
    "        generated_sequence = []\n",
    "\n",
    "        for _ in range(max_length):\n",
    "            # Predict the next token\n",
    "            decoder_outputs, h, c = self.decoder_inference.predict([target_seq] + states_value, verbose=0)\n",
    "            predicted_token = tf.argmax(decoder_outputs, axis=-1).numpy()[0, 0]  # Extract token ID\n",
    "\n",
    "            # Append the predicted token to the generated sequence\n",
    "            generated_sequence.append(predicted_token)\n",
    "            \n",
    "            # Stop if the end token is predicted\n",
    "            if predicted_token == end_token_id:  # Stop on EOS token\n",
    "                break\n",
    "            \n",
    "            # Update the target sequence and states\n",
    "            target_seq = np.array([[predicted_token]])  # Feed back into decoder\n",
    "            states_value = [h, c]  # Update states\n",
    "        generated_text = \"\"\n",
    "        # Decode the generated sequence back into a string\n",
    "        for i in generated_sequence:\n",
    "            generated_text += str(tokenizer_fr.decode([i], skip_special_tokens=False)) + ' '\n",
    "        return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callbakcs\n",
    "# Save best model\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=\"seq2seq_model.h5\",\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    save_best_only=True\n",
    ")\n",
    "# Learning rate scheduler\n",
    "learning_rate_scheduler = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",\n",
    "    factor=0.5,\n",
    "    patience=2,\n",
    "    min_lr=1e-6\n",
    ")\n",
    "\n",
    "# tdqm keras\n",
    "from tqdm.keras import TqdmCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"seq2seq_model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_input (InputLayer)     [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding_4 (Embedding)        (None, None, 100)    2000000     ['encoder_input[0][0]']          \n",
      "                                                                                                  \n",
      " decoder_input (InputLayer)     [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " encoder_lstm (LSTM)            [(None, 300),        481200      ['embedding_4[0][0]']            \n",
      "                                 (None, 300),                                                     \n",
      "                                 (None, 300)]                                                     \n",
      "                                                                                                  \n",
      " decoder (Functional)           (None, None, 20000)  8501200     ['decoder_input[0][0]',          \n",
      "                                                                  'encoder_lstm[0][1]',           \n",
      "                                                                  'encoder_lstm[0][2]']           \n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| decoder_input (InputLayer)   [(None, None)]       0           []                               |\n",
      "|                                                                                                |\n",
      "| embedding_5 (Embedding)      (None, None, 100)    2000000     []                               |\n",
      "|                                                                                                |\n",
      "| decoder_state_h (InputLayer)  [(None, 300)]       0           []                               |\n",
      "|                                                                                                |\n",
      "| decoder_state_c (InputLayer)  [(None, 300)]       0           []                               |\n",
      "|                                                                                                |\n",
      "| decoder_lstm (LSTM)          (None, None, 300)    481200      []                               |\n",
      "|                                                                                                |\n",
      "| dense_2 (Dense)              (None, None, 20000)  6020000     []                               |\n",
      "¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n",
      "==================================================================================================\n",
      "Total params: 10,982,400\n",
      "Trainable params: 10,982,400\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameters\n",
    "input_vocab_size = tokenizer_en.get_vocab_size()\n",
    "output_vocab_size = tokenizer_fr.get_vocab_size()\n",
    "CONTEXT_DIM = 300\n",
    "\n",
    "# Initialize model\n",
    "seq2seq = Seq2SeqModel(input_vocab_size, output_vocab_size, EMBEDDING_DIM, CONTEXT_DIM, MAX_SEQUENCE_LENGTH)\n",
    "seq2seq.model.summary(expand_nested=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrapper_tokenize(text):\n",
    "    text = preprocessor.apply_preprocessing(text)\n",
    "    text = tokenizer_en.encode(text, add_special_tokens=True).ids\n",
    "    return np.array(text)\n",
    "    \n",
    "\n",
    "# Tokenize sequences\n",
    "encoder_input_sequence = np.zeros((len(df), MAX_SEQUENCE_LENGTH), dtype=int)\n",
    "decoder_input_sequence = np.zeros((len(df), MAX_SEQUENCE_LENGTH), dtype=int)\n",
    "decoder_output_sequence = np.zeros((len(df), MAX_SEQUENCE_LENGTH), dtype=int)\n",
    "\n",
    "\n",
    "for j in range(len(df['en'])):\n",
    "    encoder_input_sequence[j] = wrapper_tokenize(df['en'][j])\n",
    "    decoder_input_sequence[j] = wrapper_tokenize(df['fr_input'][j])\n",
    "    decoder_output_sequence[j] = wrapper_tokenize(df['fr_output'][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((19999, 32), (19999, 32), (19999, 32))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_sequence.shape, decoder_input_sequence.shape, decoder_output_sequence.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57c1e742b92b48feb6aa29b080cfcddf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0epoch [00:00, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66f9860339c24fa799752b7d49f663ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0batch [00:00, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21e70a2b5e0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdqm_callback = TqdmCallback()\n",
    "# Compile model\n",
    "seq2seq.compile()\n",
    "\n",
    "# Train model\n",
    "seq2seq.model.fit([encoder_input_sequence, decoder_input_sequence], decoder_output_sequence, \n",
    "                  validation_split=0.1, batch_size=64, epochs=50, verbose=0,\n",
    "              callbacks=[checkpoint_callback, learning_rate_scheduler, tdqm_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"seq2seq_model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_input (InputLayer)     [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding_4 (Embedding)        (None, None, 100)    2000000     ['encoder_input[0][0]']          \n",
      "                                                                                                  \n",
      " decoder_input (InputLayer)     [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " encoder_lstm (LSTM)            [(None, 300),        481200      ['embedding_4[0][0]']            \n",
      "                                 (None, 300),                                                     \n",
      "                                 (None, 300)]                                                     \n",
      "                                                                                                  \n",
      " decoder (Functional)           (None, None, 20000)  8501200     ['decoder_input[0][0]',          \n",
      "                                                                  'encoder_lstm[0][1]',           \n",
      "                                                                  'encoder_lstm[0][2]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 10,982,400\n",
      "Trainable params: 10,982,400\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "CONTEXT_DIM = 100\n",
    "\n",
    "\n",
    "tokenizer_en = Tokenizer.from_file(\"tokenizer_en.json\")\n",
    "tokenizer_fr = Tokenizer.from_file(\"tokenizer_fr.json\")\n",
    "\n",
    "# Define hyperparameters\n",
    "input_vocab_size = tokenizer_en.get_vocab_size()\n",
    "output_vocab_size = tokenizer_fr.get_vocab_size()\n",
    "\n",
    "\n",
    "seq2seq = Seq2SeqModel(input_vocab_size, output_vocab_size, EMBEDDING_DIM, CONTEXT_DIM, MAX_SEQUENCE_LENGTH)\n",
    "seq2seq.model = tf.keras.models.load_model('seq2seq_model.h5')\n",
    "seq2seq.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "en",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "fr_input",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "fr_output",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "aec79ecf-1bc6-40c1-b822-63be8f330233",
       "rows": [
        [
         "0",
         "<sos> première moisson operating industrial scale delivering homestyle breads retailers <eos>",
         "<sos> première moisson œuvre maintenant sur une échelle industrielle par la livraison pains artisanaux aux détaillants",
         "première moisson œuvre maintenant sur une échelle industrielle par la livraison pains artisanaux aux détaillants <eos>"
        ],
        [
         "1",
         "<sos> boulangerie stméthode selling products coop atlantic retailers eastern canada 2001 gained reputation sliced fatfree sugarfree organic breads <eos>",
         "<sos> la boulangerie stméthode vend ses produits aux détaillants coop atlantique l’est canadien depuis 2001 et se démarque par des pains tranchés biologiques sans gras et sans sucre",
         "la boulangerie stméthode vend ses produits aux détaillants coop atlantique l’est canadien depuis 2001 et se démarque par des pains tranchés biologiques sans gras et sans sucre <eos>"
        ],
        [
         "2",
         "<sos> fact boulangerie stméthode won 2004 prize innovation awarded council food processing consumer products ctac seven naturobio organic breads <eos>",
         "<sos> la boulangerie stméthode d’ailleurs gagné le prix innovation ctac 2004 pour ses sept pains biologiques naturobio",
         "la boulangerie stméthode d’ailleurs gagné le prix innovation ctac 2004 pour ses sept pains biologiques naturobio <eos>"
        ],
        [
         "3",
         "<sos> breaking health food market… address health concerns fostered ageing population weston bakeries québec sells soybeanbased bread flaxseed bread rich omega3 fatty acids fatfree sugarfree breads <eos>",
         "<sos> …percée du volet santé… pour r",
         "…percée du volet santé… pour r <eos>"
        ],
        [
         "4",
         "<sos> boulangerie gadoua ltée introduced gustazzi line breads comprising seven varieties oliveoil based bread ciabatta sesame seed bread herb spice hamburger buns <eos>",
         "<sos> quant à elle la boulangerie ga",
         "quant à elle la boulangerie ga <eos>"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>fr_input</th>\n",
       "      <th>fr_output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;sos&gt; première moisson operating industrial sc...</td>\n",
       "      <td>&lt;sos&gt; première moisson œuvre maintenant sur un...</td>\n",
       "      <td>première moisson œuvre maintenant sur une éche...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;sos&gt; boulangerie stméthode selling products c...</td>\n",
       "      <td>&lt;sos&gt; la boulangerie stméthode vend ses produi...</td>\n",
       "      <td>la boulangerie stméthode vend ses produits aux...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;sos&gt; fact boulangerie stméthode won 2004 priz...</td>\n",
       "      <td>&lt;sos&gt; la boulangerie stméthode d’ailleurs gagn...</td>\n",
       "      <td>la boulangerie stméthode d’ailleurs gagné le p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;sos&gt; breaking health food market… address hea...</td>\n",
       "      <td>&lt;sos&gt; …percée du volet santé… pour r</td>\n",
       "      <td>…percée du volet santé… pour r &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;sos&gt; boulangerie gadoua ltée introduced gusta...</td>\n",
       "      <td>&lt;sos&gt; quant à elle la boulangerie ga</td>\n",
       "      <td>quant à elle la boulangerie ga &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  en  \\\n",
       "0  <sos> première moisson operating industrial sc...   \n",
       "1  <sos> boulangerie stméthode selling products c...   \n",
       "2  <sos> fact boulangerie stméthode won 2004 priz...   \n",
       "3  <sos> breaking health food market… address hea...   \n",
       "4  <sos> boulangerie gadoua ltée introduced gusta...   \n",
       "\n",
       "                                            fr_input  \\\n",
       "0  <sos> première moisson œuvre maintenant sur un...   \n",
       "1  <sos> la boulangerie stméthode vend ses produi...   \n",
       "2  <sos> la boulangerie stméthode d’ailleurs gagn...   \n",
       "3               <sos> …percée du volet santé… pour r   \n",
       "4               <sos> quant à elle la boulangerie ga   \n",
       "\n",
       "                                           fr_output  \n",
       "0  première moisson œuvre maintenant sur une éche...  \n",
       "1  la boulangerie stméthode vend ses produits aux...  \n",
       "2  la boulangerie stméthode d’ailleurs gagné le p...  \n",
       "3               …percée du volet santé… pour r <eos>  \n",
       "4               quant à elle la boulangerie ga <eos>  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "chunk_generator = pd.read_csv(\"en-fr.csv\", chunksize=25000)\n",
    "df = next(chunk_generator)\n",
    "df = df.iloc[-5000:]\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "df['en'] = df['en'].apply(preprocessor.apply_preprocessing)\n",
    "df['fr_input'] = df['fr'].apply(lambda x: preprocessor.apply_preprocessing(x, sos=True, eos=False))\n",
    "df['fr_output'] = df['fr'].apply(lambda x: preprocessor.apply_preprocessing(x, sos=False, eos=True))\n",
    "\n",
    "df.drop('fr', axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 27ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ü pôle clés fairchil demballage ciation ciation demballage ciation ciation bromure 00 concentration disposés compte compte ndnca secouent cholestéro étang ppe darachides darachides darachides chemise né né insur insur insur insur insur '"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Get the token IDs for <sos> and <eos> from your French tokenizer\n",
    "sos_token_id = tokenizer_fr.token_to_id(\"<sos>\")\n",
    "eos_token_id = tokenizer_fr.token_to_id(\"<eos>\")\n",
    "\n",
    "test_en = \"he\"\n",
    "# Then call infer with these IDs\n",
    "seq2seq.infer(test_en, sos_token_id, eos_token_id,\n",
    "              preprocess=lambda x: preprocessor.apply_preprocessing(x, sos=True, eos=True), \n",
    "              tokenizer_en=tokenizer_en, \n",
    "              tokenizer_fr=tokenizer_fr,\n",
    "              max_length=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
