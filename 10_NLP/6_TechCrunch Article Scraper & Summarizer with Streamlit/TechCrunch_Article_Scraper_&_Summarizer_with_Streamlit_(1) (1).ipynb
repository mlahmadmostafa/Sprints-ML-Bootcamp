{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M29JOaCwTCZm"
      },
      "source": [
        "# Web Scraping (BeautifulSoup)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RMF6r7s0g0qk"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin\n",
        "from transformers import pipeline\n",
        "import re\n",
        "\n",
        "# Define headers to mimic a real browser and avoid blocking\n",
        "HEADERS = {\n",
        "    \"User-Agent\": (\n",
        "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
        "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
        "        \"Chrome/115.0 Safari/537.36\"\n",
        "    )\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kUxLP4iCiM0h"
      },
      "outputs": [],
      "source": [
        "def get_article_content(url):\n",
        "    \"\"\"Fetches and returns the full content of an article given its URL.\"\"\"\n",
        "    try:\n",
        "        response = requests.get(url, headers=HEADERS)\n",
        "        if response.status_code != 200:\n",
        "            print(f\"Failed to access {url}. Status Code: {response.status_code}\")\n",
        "            return None\n",
        "\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        content_div = soup.find('div', class_='entry-content wp-block-post-content is-layout-constrained wp-block-post-content-is-layout-constrained')\n",
        "\n",
        "        if not content_div:\n",
        "            print(f\"No content found for {url}. The structure may have changed.\")\n",
        "            return None\n",
        "\n",
        "        paragraphs = content_div.find_all('p')\n",
        "        article_text = '\\n'.join(p.get_text() for p in paragraphs)\n",
        "\n",
        "        return article_text.strip() if article_text.strip() else None\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching article content from {url}: {e}\")\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "TzrNe1eqiW3U"
      },
      "outputs": [],
      "source": [
        "def get_latest_news_articles(base_url, num_articles=3):\n",
        "    \n",
        "    \"\"\"Scrapes TechCrunch’s Latest News section for a given number of articles.\"\"\"\n",
        "    articles = []\n",
        "    page = 1  # Start at page 1 for pagination\n",
        "\n",
        "    while len(articles) < num_articles:\n",
        "        url = f\"{base_url}/page/{page}\" if page > 1 else base_url\n",
        "        try:\n",
        "            response = requests.get(url, headers=HEADERS)\n",
        "        except requests.exceptions.MissingSchema:\n",
        "            print(\"Invalid URL\")\n",
        "            break\n",
        "\n",
        "        if response.status_code != 200:\n",
        "            print(f\"Failed to retrieve {url} (Status Code: {response.status_code}).\")\n",
        "            break\n",
        "        \n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Locate the query container for latest articles\n",
        "        query_container = soup.find(\"div\", class_=\"wp-block-query\")\n",
        "        if not query_container:\n",
        "            print(\"Query container not found.\")\n",
        "            break\n",
        "\n",
        "        # Find all article cards\n",
        "        article_cards = query_container.find_all(\"div\", class_=\"wp-block-techcrunch-card\")\n",
        "\n",
        "        for card in article_cards:\n",
        "            if len(articles) >= num_articles:\n",
        "                break\n",
        "\n",
        "            content_section = card.find(\"div\", class_=\"loop-card__content\")\n",
        "            if not content_section:\n",
        "                continue\n",
        "\n",
        "            # Extract title and URL\n",
        "            title_tag = content_section.find(\"h3\", class_=\"loop-card__title\")\n",
        "            link_tag = title_tag.find(\"a\", class_=\"loop-card__title-link\") if title_tag else None\n",
        "            title = link_tag.get_text(strip=True) if link_tag else \"No Title Found\"\n",
        "            article_url = urljoin(base_url, link_tag['href']) if link_tag and link_tag.has_attr('href') else None\n",
        "\n",
        "            # Extract author\n",
        "            author_tag = content_section.find(\"a\", class_=\"loop-card__author\")\n",
        "            author = author_tag.get_text(strip=True) if author_tag else \"No Author Found\"\n",
        "\n",
        "            if article_url:\n",
        "                full_content = get_article_content(article_url)\n",
        "                if full_content:  # Only add if content exists\n",
        "                    articles.append({\n",
        "                        \"title\": title,\n",
        "                        \"url\": article_url,\n",
        "                        \"author\": author,\n",
        "                        \"full_content\": full_content\n",
        "                    })\n",
        "\n",
        "        # Pagination: Go to the next page if we haven't found enough articles\n",
        "        page += 1\n",
        "\n",
        "    return articles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnzMYAlZmHrI",
        "outputId": "2b1c555e-e951-4495-cbca-81c3085ed68f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "Article 1:\n",
            "Title: The AI leaders bringing the AGI debate down to Earth\n",
            "URL: https://techcrunch.com/2025/03/19/the-ai-leaders-bringing-the-agi-debate-down-to-earth/\n",
            "Author: Maxwell Zeff\n",
            "\n",
            "Full Content:\n",
            " During a recent dinner with business leaders in San Francisco, a comment I made cast a chill over the room. I hadn’t asked my dining companions anything I considered to be extremely faux pas: simply whether they thought today’s AI could someday achieve human-like intelligence (i.e. AGI) or beyond.\n",
            "It’s a more controversial topic than you might think.\n",
            "In 2025, there’s no shortage of tech CEOs offering the bull case for how large language models (LLMs), which power chatbots like ChatGPT and Gemini, could attain human-level or even super-human intelligence over the near term. These executives argue that highly capable AI will bring about widespread — and widely distributed — societal benefits.\n",
            "For example, Dario Amodei, Anthropic’s CEO, wrote in an essay that exceptionally powerful AI could arrive as soon as 2026 and be “smarter than a Nobel Prize winner across most relevant fields.” Meanwhile, OpenAI CEO Sam Altman recently claimed his company knows how to build “superintelligent” AI, and predicted it may “massively accelerate scientific discovery.“\n",
            "However, not everyone finds these optimistic claims convincing. \n",
            "A separate cohort of AI leaders are skeptical that today’s LLMs can reach AGI — much less superintelligence — barring some novel innovations. These leaders have historically kept a low profile, but more have begun to speak up recently.\n",
            "In a piece this month, Thomas Wolf, Hugging Face’s co-founder and chief science officer, called some parts of Amodei’s vision “wishful thinking at best.” Informed by his PhD research in statistical and quantum physics, Wolf thinks that Nobel Prize-level breakthroughs don’t come from answering known questions — something that AI excels at — but rather from asking questions no one has thought to ask. \n",
            "In Wolf’s opinion, today’s LLMs aren’t up to the task.\n",
            "“I would love to see this ‘Einstein model’ out there, but we need to dive into the details of how to get there,” Wolf told TechCrunch in an interview. “That’s where it starts to be interesting.”\n",
            "Wolf said he wrote the piece because he felt there was too much hype about AGI, and not enough serious evaluation of how to actually get there. He thinks that, as things stand, there’s a real possibility AI transforms the world in the near future, but doesn’t achieve human-level intelligence or superintelligence.\n",
            "Much of the AI world has become enraptured by the promise of AGI. Those who don’t believe it’s possible are often labeled as “anti-technology,” or otherwise bitter and misinformed.\n",
            "Some might peg Wolf as a pessimist for this view, but Wolf thinks of himself as an “informed optimist” — someone who wants to push AI forward without losing grasp of reality. Certainly, he isn’t the only AI leader with conservative predictions about the technology.\n",
            "Google DeepMind CEO Demis Hassabis has reportedly told staff that, in his opinion, the industry could be up to a decade away from developing AGI — noting there are a lot of tasks AI simply can’t do today. Meta Chief AI Scientist Yann LeCun has also expressed doubts about the potential of LLMs. Speaking at Nvidia GTC on Tuesday, LeCun said the idea that LLMs could achieve AGI was “nonsense,” and called for entirely new architectures to serve as bedrocks for superintelligence.\n",
            "Kenneth Stanley, a former OpenAI lead researcher, is one of the people digging into the details of how to build advanced AI with today’s models. He’s now an executive at Lila Sciences, a new startup that raised $200 million in venture capital to unlock scientific innovation via automated labs.\n",
            "Stanley spends his days trying to extract original, creative ideas from AI models, a subfield of AI research called open-endedness. Lila Sciences aims to create AI models that can automate the entire scientific process, including the very first step — arriving at really good questions and hypotheses that would ultimately lead to breakthroughs.\n",
            "“I kind of wish I had written [Wolf’s] essay, because it really reflects my feelings,” Stanley said in an interview with TechCrunch. “What [he] noticed was that being extremely knowledgeable and skilled did not necessarily lead to having really original ideas.”\n",
            "Stanley believes that creativity is a key step along the path to AGI, but notes that building a “creative” AI model is easier said than done.\n",
            "Optimists like Amodei point to methods such as AI “reasoning” models, which use more computing power to fact-check their work and correctly answer certain questions more consistently, as evidence that AGI isn’t terribly far away. Yet coming up with original ideas and questions may require a different kind of intelligence, Stanley says.\n",
            "“If you think about it, reasoning is almost antithetical to [creativity],” he added. “Reasoning models say, ‘Here’s the goal of the problem, let’s go directly towards that goal,’ which basically stops you from being opportunistic and seeing things outside of that goal, so that you can then diverge and have lots of creative ideas.”\n",
            "To design truly intelligent AI models, Stanley suggests we need to algorithmically replicate a human’s subjective taste for promising new ideas. Today’s AI models perform quite well in academic domains with clear-cut answers, such as math and programming. However, Stanley points out that it’s much harder to design an AI model for more subjective tasks that require creativity, which don’t necessarily have a “correct” answer. \n",
            "“People shy away from [subjectivity] in science — the word is almost toxic,” Stanley said. “But there’s nothing to prevent us from dealing with subjectivity [algorithmically]. It’s just part of the data stream.”\n",
            "Stanley says he’s glad that the field of open-endedness is getting more attention now, with dedicated research labs at Lila Sciences, Google DeepMind, and AI startup Sakana now working on the problem. He’s starting to see more people talk about creativity in AI, he says — but he thinks that there’s a lot more work to be done.\n",
            "Wolf and LeCun would probably agree. Call them the AI realists, if you will: AI leaders approaching AGI and superintelligence with serious, grounded questions about its feasibility. Their goal isn’t to poo-poo advances in the AI field. Rather, it’s to kick-start big-picture conversation about what’s standing between AI models today and AGI — and super-intelligence — and to go after those blockers.\n",
            "\n",
            "================================================================================\n",
            "Article 2:\n",
            "Title: Hunted Labs lands $3M to find suspicious open source contributors\n",
            "URL: https://techcrunch.com/2025/03/19/hunted-labs-lands-3m-to-find-suspicious-open-source-contributors/\n",
            "Author: Julie Bort\n",
            "\n",
            "Full Content:\n",
            " Former NSA deputy director George Barnes has made his first investment as a venture capitalist for his new job at the VC incubation studio Red Cell Partners. It’s a $3 million seed deal in an open source cybersecurity startup called Hunted Labs, he told TechCrunch exclusively.\n",
            "Barnes spent his entire 35-year career at the spy agency, starting as an engineer, traveling from cushy assignments in places like London to war zones, he said. He worked as deputy director from 2017 to 2023. \n",
            "During that time, the “NSA had positioned itself to actually penetrate our adversaries,” he told TechCrunch. That ability to hack, “really prepares you to be a better defender,” he said adding that this is why the NSA is so good at “finding vulnerabilities and zero days.”\n",
            "It’s also why he was excited to find Hunted Labs, the brainchild of Hayden Smith. Smith previously worked on DevOps and cybersecurity for various DoD projects; during his last project for the government, Smith was working on the DoD’s big Platform One project, “which was this huge software factory,” as Smith told TechCrunch.\n",
            "Platform One allows the department’s programmers to deploy their apps faster with fewer approvals in large part by using already secured and cleared cloud or open source software (OSS). But one immediate question came up in its development: who is writing this OSS software?\n",
            "“We don’t know what connections they have to any organization or any foreign influence,” Smith said. “There really was no product or no tool out there that could help accomplish this at scale.”\n",
            "The importance of knowing software contributors became highlighted in 2024, when a lone Microsoft engineer discovered a backdoor in xz Utils, a widely used piece of software included in just about every version of Linux. The perpetrator spent years gaining trust and covering their tracks before planting this code.\n",
            "Smith wanted to create a commercial version of the background checking work he did for Platform One. So he sent cold emails to potential investors, and Barnes replied. Smith was shocked to discover he’d reached the former NSA deputy director. \n",
            "Barnes liked the idea enough to invite Hunted Labs into Red Cell’s paid, three-month “discovery” period for its incubator. An incubator is somewhat like an accelerator, only the VC is more like a co-founder, bringing the outfit’s own ideas for startups to life. \n",
            "Such deals may involve taking a larger stake than in a standard seed deal, but it offers more mentorship and support. Red Cell declined to say how much of Hunted Labs it controls.\n",
            "In that three months, Hunted Labs refined its product enough to land customers and its $3 million seed investment from Red Cell. The startup has also already landed a $1.79 million contract with the Space Development Agency, Smith said.\n",
            "Interestingly, the space agency deal didn’t come from Red Cell’s network. It came instead from the DoD connections of Smith and former DoD-project security engineer Tim Barone, who previously worked with Smith and is a co-founder of Hunted Labs, along with Smith’s wife Amanda Aguayoco. (“I have a cooling off period — that’s two years for DoD,” said Barnes, referring to why he isn’t directly involved in sales.)\n",
            "But the founders are known to many in the massive department, so unlike many Silicon Valley-born defense tech startups, they don’t need such warm intros to government buyers, anyway. \n",
            "“They are recognized professionals in their own right, and so that actually opens doors,” Barnes said.\n",
            "Hunted Labs also provides more traditional OSS software threat management, like identifying the software in use and spotting vulnerabilities in the code. In this space, it has plenty of competition like Black Duck Software, Mend.io, and Snyk.\n",
            "\n",
            "================================================================================\n",
            "Article 3:\n",
            "Title: Mortgage as an employee benefit? Kleiner Perkins leads $23.5M Series A for Multiply Mortgage\n",
            "URL: https://techcrunch.com/2025/03/19/mortgage-as-an-employee-benefit-kleiner-perkins-leads-23-5m-series-a-for-multiply-mortgage/\n",
            "Author: Mary Ann Azevedo\n",
            "\n",
            "Full Content:\n",
            " After hitting record lows at the start of the pandemic, mortgage rates began to climb in 2022 and haven’t come down significantly since. \n",
            "With 30-year mortgage rates hovering at over 6.5% today (they were as low as 2.49% in 2020!), buying a home is simply not that attainable for many people.\n",
            "One Denver-based startup is out to help change that. Founded in 2022, Multiply Mortgage originally set out to help tech employees access some of the value of their equity compensation while their employers were still private.\n",
            "But interestingly, the founders, Michael White and Gautam Gupta — alums of Square, Opendoor, DoorDash, and Uber — observed that most of the employees were using their liquidity offerings for home purchases and related expenses.\n",
            "“Homeownership has become increasingly out of reach for many Americans, and we don’t expect interest rates to fall to the levels we saw in 2020 ever again,” White told TechCrunch.\n",
            "So in July 2024, the startup altered course to offer a mortgage benefit program that helps employees of its partner companies, which include the likes of Anduril and Ramp, navigate a home purchase.\n",
            "Today, Multiply offers employees 1:1s with mortgage advisors, employee education sessions around the home purchase and financing process as well as mortgage interest rate discounts of up to .75%. The startup works with a network of 15-20 lenders to access discounted interest rates.\n",
            "For companies, claims CEO White, it’s a no brainer as they incur no costs and what he described as “low administrative overhead” to offer the program.\n",
            "“We’re really creating the category of mortgage as a financial wellness benefit,” he told TechCrunch. Traditional lenders are effectively its main competition, he said, but the startup aims to differentiate itself by a focus on financial wellness via employers in addition to its discounted rates.\n",
            "Its pivot attracted the attention of storied venture capital firm Kleiner Perkins, which just led its $23.5 million Series A, the company told TechCrunch exclusively. BoxGroup, A*, Mischief, and Workshop also participated in the financing, which brings the company’s total funding since its 2022 inception to $27 million. The company declined to reveal at which valuation this new round was raised.\n",
            "Kleiner Perkins partner Mamoon Hamid said that “attracting and retaining top talent is a focus for every great company, and providing competitive benefits and compensation programs is table stakes.” He believes that Multiply stands out because it partners directly with employers and automates traditionally time-consuming back-end processes.\n",
            "Notably, co-founder Gupta is also a general partner at investor A*, which led Multiply’s $3.5 million seed round in early 2022. He started working on the concept behind Multiply with White in late 2021 before the pair founded the company together in early 2022. \n",
            "Multiply currently operates as a broker, and is licensed to originate mortgages in 19 states. It also has broker partners in 26 additional states plus the District of Columbia. In a few months’ time, the startup plans to do actual lending itself.\n",
            "Since its pivot, the company has helped more than nearly 100 people finance their homes, White said.\n",
            "Employees can log into Multiply’s web application through their company’s email address. Once they’re validated as an employee, they can set up meetings with advisors and then access its online application, transaction dashboard, and education curriculum.\n",
            "Multiply shops its network of lenders on the employees’ behalf, finds the lowest rates, then applies its own discounts. White said Multiply is able to offer discounts in that it has automated the mortgage origination process as opposed to a more traditional “very human labor intensive process.”\n",
            "“On the technology side, we’re building the workflow automations and AI-driven tools to take a lot of the back office human labor and make the people involved significantly more efficient,” he explained. “That leads to a lower cost structure for us, and we can pass along those savings in the form of lower mortgage interest rates.”\n",
            "Multiply is not the only company that aggregates potential lenders. Others such as LendingTree do as well. But White asserts that the biggest difference between Multiply and LendingTree is the latter is more of a self-serve marketplace to find lenders and compare them. Multiply’s model is more of a concierge one that is also paired with reduced interest rates, he added.\n",
            "Presently, Multiply has 25 employees.\n",
            "It plans to use its new capital to continue investing in building out its mortgage origination platform, as well as scaling up its team of mortgage advisors and company partnerships. Today it has 23 company partners, which include a mix of public and private companies across a variety of industries.\n",
            "Multiply makes money by earning commission on mortgage originations.\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    techcrunch_url = \"https://techcrunch.com\"\n",
        "    latest_articles = get_latest_news_articles(techcrunch_url, num_articles=3)\n",
        "\n",
        "    for idx, article in enumerate(latest_articles, 1):\n",
        "        print(f\"\\n{'='*80}\\nArticle {idx}:\")\n",
        "        print(f\"Title: {article['title']}\")\n",
        "        print(f\"URL: {article['url']}\")\n",
        "        print(f\"Author: {article['author']}\")\n",
        "        print(\"\\nFull Content:\\n\", article[\"full_content\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiGC1hqUhIiV"
      },
      "source": [
        "# Text Abstractive Summarization (BART)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "SVywlOtkhKDj"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "from transformers import pipeline  # HuggingFace's transformers library for NLP tasks\n",
        "import re  # Regular expressions for text cleaning\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Cleans the input text by removing extra spaces, newlines, and non-ASCII characters.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text to be cleaned.\n",
        "\n",
        "    Returns:\n",
        "        str: The cleaned text with normalized spaces and no special characters.\n",
        "    \"\"\"\n",
        "    # Replace one or more whitespace characters (spaces, newlines, tabs) with a single space\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    # Remove non-ASCII characters (anything outside the 0x00-0x7F range)\n",
        "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
        "    # Strip leading and trailing spaces and return the cleaned text\n",
        "    return text.strip()\n",
        "\n",
        "def chunk_text(text, max_length=1024):\n",
        "    \"\"\"\n",
        "    Splits the input text into smaller chunks to handle long texts that exceed the model's token limit.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text to be chunked.\n",
        "        max_length (int): The maximum number of words per chunk. Default is 1024.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of text chunks, each containing up to `max_length` words.\n",
        "    \"\"\"\n",
        "    # Split the text into individual words\n",
        "    words = text.split()\n",
        "    # Create chunks by joining words into sublists of size `max_length`\n",
        "    chunks = []\n",
        "    for i in range(0, len(words), max_length):\n",
        "        chunks.append(' '.join(words[i:i + max_length]))\n",
        "    return chunks\n",
        "\n",
        "def summarize_text(text, summarizer, max_length=130, min_length=30):\n",
        "    \"\"\"\"\n",
        "    Summarizes the input text using the BART model. Handles long texts by chunking them first.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text to be summarized.\n",
        "        summarizer (transformers.Pipeline): The summarization pipeline (e.g., BART model).\n",
        "        max_length (int): The maximum length of the summary. Default is 130 tokens.\n",
        "        min_length (int): The minimum length of the summary. Default is 30 tokens.\n",
        "\n",
        "    Returns:\n",
        "        str: The concatenated summary of all chunks.\n",
        "    \"\"\"\n",
        "    # Split the text into manageable chunks\n",
        "    chunks = chunk_text(text)\n",
        "    summaries = []\n",
        "    # Summarize each chunk individually\n",
        "    for chunk in chunks:\n",
        "        # Generate a summary for the current chunk\n",
        "        summary = summarizer(chunk, max_length=max_length, min_length=min_length, do_sample=False)\n",
        "        # Append the summary text to the list of summaries\n",
        "        summaries.append(summary[0]['summary_text'])\n",
        "    # Join all summaries into a single string and return\n",
        "    return ' '.join(summaries)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()  # Clears unused memory\n",
        "torch.cuda.memory_allocated()  # Shows allocated memory\n",
        "torch.cuda.memory_reserved()  # Shows reserved memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHp-ggf5iuRr",
        "outputId": "c8a728e1-ad6a-49cd-8272-f47fd7f3a93b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[8], line 11\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# Load the summarization pipeline using the BART-large-CNN model\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# The model is pre-trained for summarization tasks and is available on HuggingFace's model hub\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     device \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# 0 for CUDA, -1 for CPU\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m     summarizer \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msummarization\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfacebook/bart-large-cnn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m# Iterate through the list of latest articles fetched from TechCrunch\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx, article \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(latest_articles, \u001b[38;5;241m1\u001b[39m):\n",
            "File \u001b[1;32md:\\program_files\\anaconda3\\envs\\torch_gpu\\Lib\\site-packages\\transformers\\pipelines\\__init__.py:1178\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m   1175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m processor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1176\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocessor\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m processor\n\u001b[1;32m-> 1178\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpipeline_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32md:\\program_files\\anaconda3\\envs\\torch_gpu\\Lib\\site-packages\\transformers\\pipelines\\text2text_generation.py:67\u001b[0m, in \u001b[0;36mText2TextGenerationPipeline.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 67\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_model_type(\n\u001b[0;32m     70\u001b[0m         TF_MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING_NAMES\n\u001b[0;32m     71\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     72\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING_NAMES\n\u001b[0;32m     73\u001b[0m     )\n",
            "File \u001b[1;32md:\\program_files\\anaconda3\\envs\\torch_gpu\\Lib\\site-packages\\transformers\\pipelines\\base.py:926\u001b[0m, in \u001b[0;36mPipeline.__init__\u001b[1;34m(self, model, tokenizer, feature_extractor, image_processor, processor, modelcard, framework, task, args_parser, device, torch_dtype, binary_output, **kwargs)\u001b[0m\n\u001b[0;32m    919\u001b[0m \u001b[38;5;66;03m# We shouldn't call `model.to()` for models loaded with accelerate as well as the case that model is already on device\u001b[39;00m\n\u001b[0;32m    920\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    921\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    922\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m    923\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    924\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m hf_device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    925\u001b[0m ):\n\u001b[1;32m--> 926\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    928\u001b[0m \u001b[38;5;66;03m# If the model can generate, create a local generation config. This is done to avoid side-effects on the model\u001b[39;00m\n\u001b[0;32m    929\u001b[0m \u001b[38;5;66;03m# as we apply local tweaks to the generation config.\u001b[39;00m\n\u001b[0;32m    930\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mcan_generate():\n",
            "File \u001b[1;32md:\\program_files\\anaconda3\\envs\\torch_gpu\\Lib\\site-packages\\transformers\\modeling_utils.py:3164\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3159\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[0;32m   3160\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3161\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3162\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3163\u001b[0m         )\n\u001b[1;32m-> 3164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32md:\\program_files\\anaconda3\\envs\\torch_gpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1340\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1337\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1338\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m-> 1340\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32md:\\program_files\\anaconda3\\envs\\torch_gpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
            "File \u001b[1;32md:\\program_files\\anaconda3\\envs\\torch_gpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
            "File \u001b[1;32md:\\program_files\\anaconda3\\envs\\torch_gpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:927\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    923\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    924\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    925\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    926\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 927\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    928\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    930\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
            "File \u001b[1;32md:\\program_files\\anaconda3\\envs\\torch_gpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1326\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m   1320\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[0;32m   1321\u001b[0m             device,\n\u001b[0;32m   1322\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1323\u001b[0m             non_blocking,\n\u001b[0;32m   1324\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[0;32m   1325\u001b[0m         )\n\u001b[1;32m-> 1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1331\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
            "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ],
      "source": [
        "import gc\n",
        "import torch\n",
        "import os\n",
        "\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"  # Debug CUDA issues\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Load the summarization pipeline using the BART-large-CNN model\n",
        "    # The model is pre-trained for summarization tasks and is available on HuggingFace's model hub\n",
        "    device = 0 if torch.cuda.is_available() else -1  # 0 for CUDA, -1 for CPU\n",
        "    summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", device=device)\n",
        "\n",
        "    # Iterate through the list of latest articles fetched from TechCrunch\n",
        "    for idx, article in enumerate(latest_articles, 1):\n",
        "        torch.cuda.init()\n",
        "        \n",
        "        # Print a separator line for better readability\n",
        "        print(f\"\\n{'='*80}\\nArticle {idx}:\")\n",
        "\n",
        "        # Print the article's metadata: title, URL, and author\n",
        "        print(f\"Title: {article['title']}\")\n",
        "        print(f\"URL: {article['url']}\")\n",
        "        print(f\"Author: {article['author']}\")\n",
        "\n",
        "        # Clean the article's full content to remove unnecessary spaces, newlines, and special characters\n",
        "        cleaned_text = clean_text(article[\"full_content\"])\n",
        "\n",
        "        # Summarize the cleaned text using the BART model\n",
        "        # The `summarize_text` function handles long texts by chunking them and summarizing each chunk\n",
        "        summary = summarize_text(cleaned_text, summarizer)\n",
        "\n",
        "        # Print the generated summary\n",
        "        print(\"\\nSummary:\\n\", summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2CIwuR8olbc",
        "outputId": "9a23e820-9a76-48cf-a7fd-ecf160d11b1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "\n",
        "# Function to clean text\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
        "    return text.strip()\n",
        "\n",
        "# Function to chunk text\n",
        "def chunk_text(text, max_length=1024):\n",
        "    words = text.split()\n",
        "    chunks = [' '.join(words[i:i + max_length]) for i in range(0, len(words), max_length)]\n",
        "    return chunks\n",
        "\n",
        "# Function to summarize text\n",
        "def summarize_text(text, summarizer, max_length=130, min_length=30):\n",
        "    chunks = chunk_text(text)\n",
        "    summaries = []\n",
        "    for chunk in chunks:\n",
        "        summary = summarizer(chunk, max_length=max_length, min_length=min_length, do_sample=False)\n",
        "        summaries.append(summary[0]['summary_text'])\n",
        "    return ' '.join(summaries)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ZUPXlve2micZ",
        "outputId": "114c76f3-81f5-4af4-f36d-1846c1942fc1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-03-19 16:27:20.327 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-03-19 16:27:20.328 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-03-19 16:27:20.328 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-03-19 16:27:20.329 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-03-19 16:27:20.330 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-03-19 16:27:20.330 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-03-19 16:27:20.331 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-03-19 16:27:20.331 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-03-19 16:27:20.332 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-03-19 16:27:20.332 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-03-19 16:27:20.333 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-03-19 16:27:20.334 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-03-19 16:27:20.334 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-03-19 16:27:20.334 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-03-19 16:27:20.335 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-03-19 16:27:20.335 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-03-19 16:27:20.336 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-03-19 16:27:20.336 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-03-19 16:27:20.336 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-03-19 16:27:20.337 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-03-19 16:27:20.337 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "Device set to use cuda:0\n",
            "2025-03-19 16:27:23.191 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-03-19 16:27:23.192 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-03-19 16:27:23.192 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-03-19 16:27:23.193 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-03-19 16:27:23.193 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-03-19 16:27:23.194 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        }
      ],
      "source": [
        "import streamlit as st\n",
        "\n",
        "# Streamlit app\n",
        "def main(): \n",
        "    st.title(\"TechCrunch Article Scraper & Summarizer\")\n",
        "\n",
        "    # Sidebar for navigation and controls\n",
        "    st.sidebar.title(\"Navigation & Controls\")\n",
        "    techcrunch_url = \"https://techcrunch.com\"\n",
        "    num_articles = st.sidebar.number_input(\"Number of Articles to Scrape\", min_value=1, max_value=10, value=3)\n",
        "    summary_length = st.sidebar.slider(\"Summary Length\", min_value=50, max_value=200, value=130)\n",
        "    custom_url = st.sidebar.text_input(\"Enter a custom TechCrunch article URL\")\n",
        "\n",
        "    # Load the summarization pipeline\n",
        "    summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "    # Scrape articles\n",
        "    if st.sidebar.button(\"Scrape Latest Articles\"):\n",
        "        with st.spinner(\"Scraping articles...\"):\n",
        "            articles, error = get_latest_news_articles(techcrunch_url, num_articles)\n",
        "            if error:\n",
        "                st.error(error)\n",
        "            else:\n",
        "                st.session_state.articles = articles\n",
        "\n",
        "    # Display articles in a dropdown\n",
        "    if 'articles' in st.session_state:\n",
        "        article_titles = [article['title'] for article in st.session_state.articles]\n",
        "        selected_article_title = st.sidebar.selectbox(\"Select an Article\", article_titles)\n",
        "        selected_article = next(article for article in st.session_state.articles if article['title'] == selected_article_title)\n",
        "\n",
        "        # Display original content and summary\n",
        "        st.header(\"Original Article\")\n",
        "        st.write(selected_article['full_content'])\n",
        "\n",
        "        with st.spinner(\"Generating summary...\"):\n",
        "            cleaned_text = clean_text(selected_article['full_content'])\n",
        "            summary = summarize_text(cleaned_text, summarizer, max_length=summary_length)\n",
        "            st.header(\"AI Summary\")\n",
        "            st.write(summary)\n",
        "\n",
        "        # Export options\n",
        "        st.sidebar.header(\"Export Options\")\n",
        "        if st.sidebar.button(\"Export Summary as Text File\"):\n",
        "            with open(\"summary.txt\", \"w\") as file:\n",
        "                file.write(summary)\n",
        "            st.sidebar.success(\"Summary exported as summary.txt\")\n",
        "\n",
        "    # Handle custom URL input\n",
        "    if custom_url:\n",
        "        with st.spinner(\"Fetching custom article...\"):\n",
        "            custom_article_content, error = get_article_content(custom_url)\n",
        "            if error:\n",
        "                st.error(error)\n",
        "            elif custom_article_content:\n",
        "                st.header(\"Custom Article Content\")\n",
        "                st.write(custom_article_content)\n",
        "\n",
        "                with st.spinner(\"Generating summary...\"):\n",
        "                    cleaned_text = clean_text(custom_article_content)\n",
        "                    summary = summarize_text(cleaned_text, summarizer, max_length=summary_length)\n",
        "                    st.header(\"AI Summary\")\n",
        "                    st.write(summary)\n",
        "\n",
        "                # Export options for custom article\n",
        "                st.sidebar.header(\"Export Options\")\n",
        "                if st.sidebar.button(\"Export Custom Summary as Text File\"):\n",
        "                    with open(\"custom_summary.txt\", \"w\") as file:\n",
        "                        file.write(summary)\n",
        "                    st.sidebar.success(\"Custom summary exported as custom_summary.txt\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Testing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Google has officially unveiled the Pixel 9a smartphone, its new midrange phone that will retail for $499. The A-series smartphone’s biggest change is its appearance, as the latest model ditches the camera bar on the back of the phone.\n",
            "The smartphone is getting a chip upgrade, as the new model features Google’s Tensor G4 processor, while its predecessor had a G3 chip. The 9a also promises over 30 hours of battery life, features 8GB of RAM, and comes with either 128GB or 256GB of storage.\n",
            "It will be available beginning in April and comes in four colorways: Peony, Iris, Porcelain, and Obsidian.\n",
            "In addition, the smartphone features an upgraded 6.3-inch Actua display, which Google says is the brightest display on an A-series ever. For comparison, the new display is 35% brighter than the Pixel 8a at 2700 nits. The 9a also features a 120 Hz adaptive refresh rate for a smoother experience.\n",
            "The smartphone’s upgraded dual rear camera system has both a 13MP ultrawide camera and a 48MP main camera. It also features Macro Focus for the first time on an A-series device, which will allow for close-up details. \n",
            "The 9a is equipped with AI-powered photography features, including Add Me, Best Take, Magic Editor, Magic Eraser, Audio Magic Eraser, Night Sight, Astrophotography, and the new Panorama with Night Sight.\n",
            "Google also notes that the 9a is the company’s most durable A-series phone yet, as “its upgraded IP68 water and dust resistance means it can withstand spills, drops, and dings.”\n",
            "Wednesday’s announcement comes a few weeks after Apple launched its new $599 iPhone 16e.\n",
            "Zoox, Amazon’s autonomous driving unit, has issued a voluntary recall for 258 vehicles due to issues with its autonomous driving system that could cause unexpected hard breaking. \n",
            "The recall follows a preliminary investigation from the National Highway Traffic Safety Administration (NHTSA) last May after the agency received two reports of incidents in which motorcyclists collided with the back of Zoox vehicles — Toyota Highlanders equipped with Zoox technology. The initial investigation found that in both cases, the Zoox vehicles were operating with their autonomous systems engaged.  \n",
            "Zoox is recalling only vehicles equipped with software versions released before November 5, 2024. The company said in a NHTSA report that it addressed the issue with a software update by November 7, 2024. \n",
            "TechCrunch has reached out to learn whether Zoox’s Toyota Highlanders were the only vehicles affected, or if Zoox’s upgrade went out to its purpose-built robotaxis, as well.\n",
            "Zoox is currently testing a limited number of its custom robotaxis, built without a steering wheel or pedals, in San Francisco and Las Vegas. Last June, Zoox shared plans to start testing its technology in Austin and Miami.\n",
            "The recall comes after a few of years of heightened scrutiny from regulators following a safety incident with Cruise in 2023, which has since been shuttered and absorbed into its parent company General Motors.\n",
            "Last year, Waymo issued a voluntary recall of 672 Jaguar I-Pace robotaxis following several reported safety incidents, including when one collided with a telephone pole.\n",
            "Mobile networks continue to be a major target for cybersecurity breaches, and Chinese hacking group Salt Typhoon‘s persistent attacks on multiple carriers are only the latest known examples. \n",
            "The mobile carrier startup Cape is taking a novel approach to addressing the problem: it has built a service it says can provide a more secure, private alternative because it doesn’t collect any data on you at all — even its website doesn’t have a cookie gate. Today, Cape is announcing a few big developments in its efforts. \n",
            "The Washington, DC-based company — founded by a former head of Palantir’s national security business who previously worked in the U.S. Army’s special forces — is releasing an open beta of its MVNO mobile service, which comes in the form of a $99/month subscription plan; it has inked a partnership with Proton — the provider of encrypted e-mail, VPN and cloud services; and it’s raised $30 million more in equity and debt funding. \n",
            "The $30 million is broken down as further $15 million in equity tacked on to its Series B from A*, Costanoa, Point72, XYZ Ventures. A further $15 million is coming in the form of a debt facility from Silicon Valley Bank. This brings the equity part of its Series B to $55 million, with the original $40 million announced in April 2024 led by Andreessen Horowitz. \n",
            "Cape is not disclosing valuation, but it’s notable that the funding is coming at a time when startups building military, defense, and security services are getting increased focused and priority at a time when geopolitics are shifting. \n",
            "While many of those shifts are playing out at a much higher level involving wars, espionage against officers and officials, and major contacts between outsized industrial entities, Cape’s products and its growth are one of the rare examples of how some of that evolution is playing out at a consumer level. \n",
            "That’s not to say that all of Cape’s products are for everyday people. The new plan comes on the heels of the company last year emerging from stealth with $61 million in funding, launching a $1,500 phone called the Obscura, which was designed expressly for military and government people and others like them: “those facing elevated threats” in the words of the company. Then in January 2025, Cape made its first move to launch a service for consumers. All the open slots in that closed beta filled up in four hours. \n",
            "CEO John Doyle, who co-founded the company with Nicholas Espinoza (who is the head of R&D), said the rapid pace of sign-ups pointed to “a lot of interest from the broader consumer market, folks who have a general desire to take back some of their privacy, take back control of the digital identity as they connect to global networks, but maybe don’t want to invest at the level required to buy an Obscura phone,” and that’s what led to today’s open beta. \n",
            "In addition to a commitment not to track or sell data, the plan includes unlimited voice minutes, texts and data (but no voice over WiFi yet, Doyle said; that’s still coming), as well as encrypted voicemail. \n",
            "As part of its plan, Cape also provides protection against two other emerging cellular threats. The first of these is protection from SIM swapping, using cryptographic protection to keep someone from hijacking your number. \n",
            "And it offers what it describes as “advanced signalling protection” — which is in reference to side channel attacks via telephony signally protocol Signalling System 7. Tracking via SS7 has been a known issue for years, but in December 2024 it was highlighted by the U.S. government as a particular issue for sensitive calls, texts and data, which it said could be accessed by spies from military and other personnel using the protocol.\n",
            "Doyle added that general availability for its phone plan will come later this year. Currently, there are under 1,000 users on its closed beta, and a couple of hundred Obscura phone owners. \n",
            "Cape’s service is anchored on UScellular, and the plan is to introduce roaming services to its users to extend to other countries, as well as to introduce MNVO-based plans in other countries. \n",
            "Europe has proven to be a big market for privacy-first services, as well as those that provide alternatives to engaging with Big Tech. That’s led to apps like Signal climbing to the top of the app stores in at least one European market. That could present an interesting addressable market for a startup like Cape.\n",
            "Cape is leaning into that European penchant for privacy in another way in the meantime. Just as mainstream carriers like to link up in marketing partnerships with buzzy consumer services to drive more sign-ups — one recent example being T-Mobile inking a partnership with Perplexity for an “AI Phone” — Cape is doing the same with like-minded privacy-first companies. First out the door is a deal with Switzerland-based Proton to drive sign-ups to the latter company’s premium (paid) offerings. Those who sign up to a Cape $99/month phone plan can pay $1 to add on on six months of Proton’s Unlimited plan, which includes encrypted cloud storage, VPN, expanded secure e-mail and more.\n",
            "“We did a really pretty rigorous survey of the field and determine we think Proton is the clear leader in terms of credibility and how advanced their tech is and the way they’re approaching the problems [of privacy],” Doyle said. He described the offer as “the first version” of a partnership between the two companies.\n",
            "TechCrunch Sessions: AI is nearly upon us, and you have the power to decide who you want to see lead a dynamic breakout session. We have whittled speaker applications down to six incredible finalists to take the stage on June 5 in Zellerbach Hall at UC Berkeley. Who do you want to see share their wisdom with 1,200 AI leaders and enthusiasts? The choice is now in your hands.\n",
            "You can vote in Audience Choice through March 21 at 11:59 p.m. PT. But make your vote count because you can only choose one speaker!\n",
            "Don’t forget to lock in your ticket now to save up to $210 and guarantee your seat in the most-voted session and top AI discussions. Explore cutting-edge innovations, connect with industry leaders, and dive into the AI revolution!\n",
            "Guide to Monetizing GenAI in Enterprise Applications — Mahesh Chayel, Product Management Lead, Meta\n",
            "This session from Meta’s Mahesh Chayel will let you discover methods to monetize generative AI in enterprise apps — from API licensing to premium features and more. Vote for this session to better understand ethical issues, pinpoint valuable use cases, and create lasting AI revenue streams.\n",
            "Who Is Coding Our Future? — Cristina Mancini, CEO, Black Girls Code\n",
            "Women make up just 27% of tech roles, and Black women make up less than 2%. Cristina Mancini of Black Girls Code will lead a discussion about how greater inclusion can reshape tech — as well as create a more equitable, innovative future for all.\n",
            "Behind Your Firewall: Secure Generative AI for Regulated Enterprises — Yann Stoneman, Staff Solutions Architect, Cohere AI\n",
            "This session from Cohere AI’s Yann Stoneman will teach you how to deploy secure, private AI in healthcare or finance without the cloud. You’ll be able to see real-world use cases and demos, get expert tips, and learn how to manage data privacy while building AI solutions via a Q&A.\n",
            "The AI Policy Playbook: What Startups Need to Know — Hua Wang, Executive Director, Global Innovation Forum\n",
            "While AI is transforming startups, policy and global expansion remain challenging. This session from Hua Wang will explore AI-driven tools for digital trade, compliance, and scaling. Vote for this session to gain insights on policies, data regulations, and international growth.\n",
            "Implementing Secure Generative AI with Guardrails — Hardik Vasa, Senior Solutions Architect, AWS\n",
            "AWS’ Hardik Vasa will teach you how to safeguard your generative AI with Amazon Bedrock Guardrails. Choose this session and discover how to block harmful content, filter responses, and protect sensitive information to ensure secure, compliant AI interactions for your business.\n",
            "A Big Tech Banker-Turned-Investor’s Unique Lens on Identifying Exceptional AI Founders — Marcie Vu, Partner, Greycroft\n",
            "Greycroft partner Marcie Vu will share her unique framework for spotting standout AI founders and actionable strategies for building enduring AI companies, beyond the hype, for true market success.\n",
            "Have a favorite speaker or session? Vote today for their chance to take the stage in a breakout session at TechCrunch Sessions: AI!\n",
            "Google announced on Wednesday that kids with Android phones can now tap to pay at stores using Google Wallet in the United States, United Kingdom, Australia, Spain, and Poland. Parents and guardians in these countries can now allow their children to access digital payments on their Android devices with supervision.\n",
            "Kids can also use Google Wallet to access supported passes, like event tickets, library cards, and gift cards. \n",
            "Google notes that a payment card can only be added with parental consent, and that parents will receive an email whenever their child makes a transaction. Plus, parents can use Family Link, Google’s parental control app, to track their child’s recent purchases, remove payment cards, and turn off access to passes. \n",
            "It’s worth noting that children will not be able to use Google Wallet to pay for online purchases. \n",
            "To get started, a child and their parent need to navigate to the Wallet app on the child’s Android phone and tap the “Add to Wallet” on the bottom left of their screen. Then, they need to select the “Payment card” option and tap “New credit or debit card.”\n",
            "The app will then prompt the parent to verify that they are a parent by logging into their Google Account. They can then add a credit or debit card. Once they have done so, the child can start using the Wallet app for in-store purchases. \n",
            "The launch doesn’t come as a surprise, as Google announced last month that it would be introducing the feature this spring. Google did not comment when asked about its plans to expand the feature to more regions.\n",
            "With today’s announcement, Google is catching up with Apple, which already allows children to use Apple Pay for in-store purchases through its Apple Cash Family service.\n",
            "Booking an ad campaign with social media influencers is currently not exactly easy. For starters, influencers’ approaches to marketing can be unconventional, and there’s no standard way to engage with them. On the other side, marketing agencies that employ hosts of people to book and track brand campaigns are limited by how many influencers they can engage at any one time.\n",
            "Put simply, the creator marketing ecosystem is being held back in many ways by the old-world ad/marketing agency model. Wouldn’t it be easier if an AI chatbot could do all the heavy lifting, interacting naturally with an influencer via a platform that’s able to scale across hundreds of ad campaigns? \n",
            "That’s the idea behind the company Agentic Marketing Technologies (AMT), which has raised $3.5 million in a seed funding round led by San Francisco-based VC NFX.\n",
            "AMT works by getting its AI agent, dubbed Lyra, to talk to influencers using natural language, helping with tasks like booking campaigns, tracking results, making payments, and answering queries. The company claims Lyra can also autonomously find influencers that match a campaign’s goals.\n",
            "Tom Hollands, co-founder and CEO of AMT, told TechCrunch he became familiar with the challenge after managing influencer marketing budgets himself. Co-founder Christian Johnston (CTO) previously built adtech data infrastructure.\n",
            "“The problem in the market today is that the way that you scale influencer marketing is you hire 22-year-olds who are working 20 hours a day, and you load them up with as many partnerships as possible until they break,” Hollands said. “They can’t remember the names of the influencers that they message, and they spend all their time manually following up,” said Hollands.\n",
            "AMT employs a combination of AI models, including OpenAI’s for general use, Google’s Gemini for multimodal (i.e. analyzing creators’ videos), and Hume AI’s for “tone.” Hollands added, “We use the best model for each task, independent of the provider.”\n",
            "Hollands argues that because AI can actually “watch” and “understand” influencer content to a degree, it can deliver a much more personalized experience. \n",
            "“[AI] can actually understand the tone of voice of each influencer,” Hollands said. “It means it’s possible to communicate with one influencer across multiple brands the way [a] partnerships manager would because it has a relationship history of all of these different conversations.”\n",
            "Launched three months ago, AMT, which is relocating from London to San Francisco, says it has already attracted customers such as Le Petit Luetier, Neoplants, and Wild.\n",
            "The influencer market is projected to be worth  $266.92 billion this year, and traditional influencer marketing SaaS platforms like GRIN and Upfluence, as well as marketplaces like ShopMy and Agentio, require human involvement to run campaigns. These typically charge by seat. AMT’s AI-driven approach, obviously, has drastically different economics, given that far fewer humans are involved.AMT says it usually takes nine hours of manual work to secure a single influencer partnership, but just five minutes with its platform. \n",
            "In a statement, Pete Flint, general partner at NFX, added: “AI is fundamentally reshaping industries, and marketing is no exception. AMT’s approach is unique in that it isn’t just building tools, it’s replacing human work with AI, making it an inevitable part of the marketing stack for brands worldwide.”\n",
            "The governments of Australia, Canada, Cyprus, Denmark, Israel, and Singapore are likely customers of Israeli spyware maker Paragon Solutions, according to a new technical report by a renowned digital security lab.\n",
            "On Wednesday, The Citizen Lab, a group of academics and security researchers housed at the University of Toronto that has investigated the spyware industry for more than a decade, published a report about the Israeli-founded surveillance startup, identifying the six governments as “suspected Paragon deployments.”\n",
            "At the end of January, WhatsApp notified around 90 users that the company believed were targeted with Paragon spyware, prompting a scandal in Italy, where some of the targets live. \n",
            "Paragon has long tried to distinguish itself from competitors, such as NSO Group — whose spyware has been abused in several countries — by claiming to be a more responsible spyware vendor. In 2021, an unnamed senior Paragon executive told Forbes that authoritarian or non-democratic regimes would never be its customers. \n",
            "In response to the scandal prompted by the WhatsApp notifications in January, and in what was perhaps an attempt to bolster its claims about being a responsible spyware vendor, Paragon’s executive chairman John Fleming told TechCrunch that the company “licenses its technology to a select group of global democracies — principally, the United States and its allies.”\n",
            "Israeli news outlets reported in late 2024 that U.S. venture capital AE Industrial Partners had acquired Paragon for at least $500 million upfront.\n",
            "In the report out Wednesday, Citizen Lab said it was able to map the server infrastructure used by Paragon for its spyware tool, which the vendor codenamed Graphite, based on “a tip from a collaborator.” \n",
            "Starting from that tip, and after developing several fingerprints capable of identifying associated Paragon servers and digital certificates, Citizen Lab’s researchers found several IP addresses hosted at local telecom companies. Citizen Lab said it believes these are servers belonging to Paragon customers, in part based on the initials of the certificates, which seem to match the names of the countries the servers are located in. \n",
            "According to Citizen Lab, one of the fingerprints developed by its researchers led to a digital certificate registered to Graphite, in what appears to be a significant operational mistake by the spyware maker.\n",
            "“Strong circumstantial evidence supports a link between Paragon and the infrastructure we mapped out,” Citizen Lab wrote in the report. \n",
            "“The infrastructure we found is linked to webpages entitled ‘Paragon’ returned by IP addresses in Israel (where Paragon is based), as well as a TLS certificate containing the organization name ‘Graphite’,” the report said.\n",
            "Citizen Lab noted that its researchers identified several other codenames, indicating other potential governmental customers of Paragon. Among the suspected customer countries, Citizen Lab singled out Canada’s Ontario Provincial Police (OPP), which specifically appears to be a Paragon customer given that one of the IP addresses for the suspected Canadian customer is linked directly to the OPP.\n",
            " \n",
            "Contact Us\n",
            "\t\t\tDo you have more information about Paragon, and this spyware campaign? From a non-work device, you can contact Lorenzo Franceschi-Bicchierai securely on Signal at +1 917 257 1382, or via Telegram and Keybase @lorenzofb, or email. You also can contact TechCrunch via SecureDrop.\t\t\n",
            "\n",
            "TechCrunch reached out to spokespeople for the following governments: Australia, Canada, Cyprus, Denmark, Israel, and Singapore. TechCrunch also contacted the Ontario Provincial Police. None of the representatives responded to our requests for comment. \n",
            "When reached by TechCrunch, Paragon’s Fleming said that Citizen Lab reached out to the company and provided “a very limited amount of information, some of which appears to be inaccurate.” \n",
            "Fleming added: “Given the limited nature of the information provided, we are unable to offer a comment at this time.” Fleming did not respond when TechCrunch asked what was inaccurate about Citizen Lab’s report, nor responded to questions about whether the countries identified by Citizen Lab are Paragon customers, or the status of its relationship with its Italian customers. \n",
            "Citizen Lab noted that all the people that were notified by WhatsApp, who then reached out to the organization to have their phones analyzed, used an Android phone. This allowed the researchers to identify a “forensic artifact” left by Paragon’s spyware, which the researchers called “BIGPRETZEL.”\n",
            "Meta spokesperson Zade Alsawah told TechCrunch in a statement that the company “can confirm that we believe that the indicator Citizen Lab refers to as BIGPRETZEL is associated with Paragon.” \n",
            "“We’ve seen first-hand how commercial spyware can be weaponized to target journalists and civil society, and these companies must be held accountable,” read Meta’s statement. “Our security team is constantly working to stay ahead of threats, and we will continue working to protect peoples’ ability to communicate privately.”\n",
            "Given that Android phones do not always preserve certain device logs, Citizen Lab noted that it’s likely more people were targeted by the Graphite spyware, even if there was no evidence of Paragon’s spyware on their phones. And for the people who were identified as victims, it’s not clear if they were targeted on previous occasions.\n",
            "Citizen Lab also noted that Paragon’s Graphite spyware targets and compromises specific apps on the phone — without needing any interaction from the target — rather than compromising the wider operating system and the device’s data. In the case of Beppe Caccia, one of the victims in Italy, who works for an NGO that helps migrants, Citizen Lab found evidence that the spyware infected two other apps on his Android device, without naming the apps.\n",
            "Targeting specific apps as opposed to the device’s operating system, Citizen Lab noted, may make it harder for forensic investigators to find evidence of a hack, but may give the app makers more visibility into spyware operations. \n",
            "“Paragon’s spyware is trickier to spot than competitors like [NSO Group’s] Pegasus, but, at the end of the day, there is no ‘perfect’ spyware attack,” Bill Marczak, a senior researcher at Citizen Lab, told TechCrunch. “Maybe the clues are in different places than we’re used to, but with collaboration and information sharing, even the toughest cases unravel.” \n",
            "Citizen Lab also said it analyzed the iPhone of David Yambio, who works closely with Caccia and others at his NGO. Yambio received a notification from Apple about his phone being targeted by mercenary spyware, but the researchers couldn’t find evidence that he was targeted with Paragon’s spyware. \n",
            "Apple did not respond to a request for comment.\n",
            "Nearly 10 years after filing to go public on the New York Stock Exchange (NYSE) and Toronto Stock Exchange, Canadian e-commerce platform Shopify has announced that it’s transitioning its U.S. listing to the Nasdaq.\n",
            "In a filing with the Securities and Exchange Commission (SEC) on Wednesday, Shopify said it is removing its Class A shares from the NYSE at the close of trading on Friday, March 28, with trading recommencing on the Nasdaq starting Monday, March 31. The company added that its existing listing on the Toronto Stock Exchange will stay as is, and its ticker symbol will also remain SHOP on both exchanges.\n",
            "Shopify gave no explicit reason in the filing for changing its U.S. stock exchange listing, however a spokesperson for the company told TechCrunch: “We’re excited to join the Nasdaq community and be listed among the most innovative tech companies in the world.”\n",
            "Shopify last month reported a reasonably strong Q4 2024, exceeding estimates with its revenue growing 31% year-on-year to $2.8 billion. The company’s market cap currently sits at $121 billion, up 55% from the same period last year.\n",
            "There’s a controversy brewing over “AI-generated” studies submitted to this year’s ICLR, a long-running academic conference focused on AI.\n",
            "At least three AI labs — Sakana, Intology, and Autoscience — claim to have used AI to generate studies that were accepted to ICLR workshops. At conferences like ICLR, workshop organizers typically review studies for publication in the conference’s workshop track.\n",
            "Sakana informed ICLR leaders before it submitted its AI-generated papers and obtained the peer reviewers’ consent. The other two labs — Intology and Autoscience — did not, an ICLR spokesperson confirmed to TechCrunch.\n",
            "Several AI academics took to social media to criticize Intology and Autoscience’s stunts as a co-opting of the scientific peer review process.\n",
            "“All these AI scientist papers are using peer-reviewed venues as their human evals, but no one consented to providing this free labor,” wrote Prithviraj Ammanabrolu, an assistant computer science professor at UC San Diego, in an X post. “It makes me lose respect for all those involved regardless of how impressive the system is. Please disclose this to the editors.”\n",
            "As the critics noted, peer review is a time-consuming, labor-intensive, and mostly volunteer ordeal. According to one recent Nature survey, 40% of academics spend two to four hours reviewing a single study. That work has been escalating. The number of papers submitted to the largest AI conference, NeurIPS, grew to 17,491 last year, up 41% from 12,345 in 2023.\n",
            "Academia already had an AI-generated copy problem. One analysis found that between 6.5% and 16.9% of papers submitted to AI conferences in 2023 likely contained synthetic text. But AI companies using peer review to effectively benchmark and advertise their tech is a relatively new occurrence.  \n",
            "“[Intology’s] papers received unanimously positive reviews,” Intology wrote in a post on X touting its ICLR results. In the same post, the company went on to claim that workshop reviewers praised one of its AI-generated study’s “clever idea[s].” \n",
            "Academics didn’t look kindly on this. \n",
            "Ashwinee Panda, a postdoctoral fellow at the University of Maryland, said in an X post that submitting AI-generated papers without giving workshop organizers the right to refuse them showed a “lack of respect for human reviewers’ time.”\n",
            "“Sakana reached out asking whether we would be willing to participate in their experiment for the workshop I’m organizing at ICLR,” Panda added, “and I (we) said no […] I think submitting AI papers to a venue without contacting the [reviewers] is bad.”\n",
            "Not for nothing, many researchers are skeptical that AI-generated papers are worth the peer review effort.\n",
            "Sakana itself admitted that its AI made “embarrassing” citation errors, and that only one out of the three AI-generated papers the company chose to submit would’ve met the bar for conference acceptance. Sakana withdrew its ICLR paper before it could be published in the interest of transparency and respect for ICLR convention, the company said.\n",
            "Alexander Doria, the co-founder of AI startup Pleias, said that the raft of surreptitious synthetic ICLR submissions pointed to the need for a “regulated company/public agency” to perform “high-quality” AI-generated study evaluations for a price.\n",
            "“Evals [should be] done by researchers fully compensated for their time,” Doria said in a series of posts on X. “Academia is not there to outsource free [AI] evals.”\n",
            "Prezent, a startup empowering customers to build slide decks using generative AI, has raised $20 million as it further develops and refines its AI models for different use cases and expands into new markets.\n",
            "AI has many different applications — one of which is generating decks for business presentations, as it turns out. Off-the-shelf models tend to be not very good at this because they don’t understand industry-specific language and jargon. That’s where Prezent’s technology comes in.\n",
            "Los Altos-based Prezent, which has a subsidiary in Bengaluru, was founded in 2021 by Rajat Mishra, who previously worked at companies including Cisco and McKinsey. Mishra says he struggled to overcome stuttering and speech impediments at a young age, which drove his interest in communication tech. \n",
            "“The idea [for Prezent] was, wouldn’t it be cool if we could build an AI platform that democratizes business communication and makes everyone a great business communicator?” Mishra told TechCrunch in an interview.\n",
            "Prezent’s platform has users upload their assets and documents, such as Excel files, PDFs, and links, and give Prezent’s AI assistant, called Astrid, context about the company for which they’re creating a presentation. Users can also include team abbreviations and specific terminology, as well as additional preferences and requirements.\n",
            "Prezent uses openly available AI models fine-tuned on proprietary data, including a dataset of 2 million slide decks, to power its platform, Mishra says. The startup also builds AI models for specific applications, like recommending layouts users should use for particular decks.\n",
            "For companies on a tight deadline who want a little human polish, Prezent offers an expedited service that turns draft documents into what the company describes as “professional-grade” presentations. The service employs a combination of AI and human reviewers, including consultants and designers, to overnight spruced-up, finalized drafts.\n",
            "Prezent claims to serve around 150 Fortune 2000 companies, and targets customers in the biopharma and tech industries. Mishra says that the new funding, an all-equity investment and an extension of Prezent’s April 2022 Series A, will help Prezent go after potential clients in financial services and manufacturing and expand its operations to Europe, Japan, and Singapore.\n",
            "Prezent already has several customers in Europe and is looking to expand into Southeast Asia in Q2 of this year. Last year, the company generated over $10 million in annual recurring revenue, Mishra told TechCrunch.\n",
            "Prezent also plans to launch APIs to allow developers to create presentations directly from chatbots, apps, search engines, and more using its models.\n",
            "“DeepSeek recently showed us that AI companies don’t need these massive funding rounds to make a big impact any longer,” Mishra told TechCrunch, adding that Prezent currently has “close to 200” employees, 75% of whom work remotely and are based in India.\n",
            "Prezent’s latest investment — which values Prezent at “well over” $100 million, according to Mishra — was led by Greycroft with participation from Zoom Ventures, Emergent Ventures, and WestWave. New investors True Global Ventures (TGV), Manulife Ventures, and Alumni Ventures also joined the tranche.\n"
          ]
        }
      ],
      "source": [
        "# Test get_latest_news_articles\n",
        "articeles = get_latest_news_articles(\"https://techcrunch.com\", num_articles=10)\n",
        "\n",
        "# Test get_article_content\n",
        "for article in articeles:\n",
        "    print(get_article_content(article[\"url\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Invalid URL\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Testin error handling\n",
        "get_latest_news_articles(\"1\", num_articles=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.microsoft.datawrangler.viewer.v0+json": {
              "columns": [
                {
                  "name": "index",
                  "rawType": "int64",
                  "type": "integer"
                },
                {
                  "name": "texts",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "cleaned",
                  "rawType": "object",
                  "type": "string"
                }
              ],
              "conversionMethod": "pd.DataFrame",
              "ref": "99fc1bfe-a246-4abf-8810-2890714b5493",
              "rows": [
                [
                  "0",
                  "Café au lait ☕ is tasty!",
                  "Caf  au lait   is tasty!"
                ],
                [
                  "1",
                  "你好, 世界! (Hello, World!)",
                  ",  ! (Hello, World!)"
                ],
                [
                  "2",
                  "Jalapeño 🌶️ is spicy!",
                  "Jalape o   is spicy!"
                ],
                [
                  "3",
                  "I ♥ Python!",
                  "I   Python!"
                ],
                [
                  "4",
                  "El Niño affects weather.",
                  "El Ni o affects weather."
                ],
                [
                  "5",
                  "Привет! Как дела?",
                  "!    ?"
                ],
                [
                  "6",
                  "München is in Germany.",
                  "M nchen is in Germany."
                ],
                [
                  "7",
                  "São Paulo is a big city.",
                  "S o Paulo is a big city."
                ],
                [
                  "8",
                  "Résumé or CV?",
                  "R sum  or CV?"
                ],
                [
                  "9",
                  "Tokyo 東京 is amazing!",
                  "Tokyo   is amazing!"
                ]
              ],
              "shape": {
                "columns": 2,
                "rows": 10
              }
            },
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>texts</th>\n",
              "      <th>cleaned</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Café au lait ☕ is tasty!</td>\n",
              "      <td>Caf  au lait   is tasty!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>你好, 世界! (Hello, World!)</td>\n",
              "      <td>,  ! (Hello, World!)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Jalapeño 🌶️ is spicy!</td>\n",
              "      <td>Jalape o   is spicy!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>I ♥ Python!</td>\n",
              "      <td>I   Python!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>El Niño affects weather.</td>\n",
              "      <td>El Ni o affects weather.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Привет! Как дела?</td>\n",
              "      <td>!    ?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>München is in Germany.</td>\n",
              "      <td>M nchen is in Germany.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>São Paulo is a big city.</td>\n",
              "      <td>S o Paulo is a big city.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Résumé or CV?</td>\n",
              "      <td>R sum  or CV?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Tokyo 東京 is amazing!</td>\n",
              "      <td>Tokyo   is amazing!</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                      texts                   cleaned\n",
              "0  Café au lait ☕ is tasty!  Caf  au lait   is tasty!\n",
              "1   你好, 世界! (Hello, World!)      ,  ! (Hello, World!)\n",
              "2     Jalapeño 🌶️ is spicy!      Jalape o   is spicy!\n",
              "3               I ♥ Python!               I   Python!\n",
              "4  El Niño affects weather.  El Ni o affects weather.\n",
              "5         Привет! Как дела?                    !    ?\n",
              "6    München is in Germany.    M nchen is in Germany.\n",
              "7  São Paulo is a big city.  S o Paulo is a big city.\n",
              "8             Résumé or CV?             R sum  or CV?\n",
              "9      Tokyo 東京 is amazing!       Tokyo   is amazing!"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Test clean_text\n",
        "import pandas as pd\n",
        "texts = [\n",
        "    \"Café au lait ☕ is tasty!\",      # Removes \"é\" and \"☕\"\n",
        "    \"你好, 世界! (Hello, World!)\",    # Removes \"你好, 世界!\"\n",
        "    \"Jalapeño 🌶️ is spicy!\",         # Removes \"ñ\" and \"🌶️\"\n",
        "    \"I ♥ Python!\",                   # Removes \"♥\"\n",
        "    \"El Niño affects weather.\",       # Removes \"ñ\"\n",
        "    \"Привет! Как дела?\",              # Removes \"Привет! Как дела?\"\n",
        "    \"München is in Germany.\",         # Removes \"ü\"\n",
        "    \"São Paulo is a big city.\",       # Removes \"ã\"\n",
        "    \"Résumé or CV?\",                  # Removes \"é\"\n",
        "    \"Tokyo 東京 is amazing!\",         # Removes \"東京\"\n",
        "]\n",
        "\n",
        "cleaned_texts = [clean_text(text) for text in texts]\n",
        "\n",
        "df = pd.DataFrame({\"texts\":texts,\n",
        "                   \"cleaned\":cleaned_texts})\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\t263\t\n",
            "1\t244\t\n",
            "1\t916\t\n",
            "1\t495\t\n",
            "1\t294\t\n",
            "1\t533\t\n",
            "2\t1024\t30\t\n",
            "1\t183\t\n",
            "1\t538\t\n",
            "1\t496\t\n"
          ]
        }
      ],
      "source": [
        "# Test chunk_text\n",
        "# Outputs a list of chunks each one up to the max_length\n",
        "for article in articeles:\n",
        "    print(len(chunk_text(article[\"full_content\"])), end = \"\\t\")\n",
        "    for chunk in chunk_text(article[\"full_content\"], max_length=1024):\n",
        "        print(len(chunk.split()), end = \"\\t\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[31], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Test summarize_text\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m summarizer \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msummarization\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfacebook/bart-large-cnn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m article \u001b[38;5;129;01min\u001b[39;00m articeles:\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(summarize_text(article[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfull_content\u001b[39m\u001b[38;5;124m\"\u001b[39m], summarizer))\n",
            "File \u001b[1;32md:\\program_files\\anaconda3\\envs\\torch_gpu\\Lib\\site-packages\\transformers\\pipelines\\__init__.py:1178\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m   1175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m processor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1176\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocessor\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m processor\n\u001b[1;32m-> 1178\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpipeline_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32md:\\program_files\\anaconda3\\envs\\torch_gpu\\Lib\\site-packages\\transformers\\pipelines\\text2text_generation.py:67\u001b[0m, in \u001b[0;36mText2TextGenerationPipeline.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 67\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_model_type(\n\u001b[0;32m     70\u001b[0m         TF_MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING_NAMES\n\u001b[0;32m     71\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     72\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING_NAMES\n\u001b[0;32m     73\u001b[0m     )\n",
            "File \u001b[1;32md:\\program_files\\anaconda3\\envs\\torch_gpu\\Lib\\site-packages\\transformers\\pipelines\\base.py:926\u001b[0m, in \u001b[0;36mPipeline.__init__\u001b[1;34m(self, model, tokenizer, feature_extractor, image_processor, processor, modelcard, framework, task, args_parser, device, torch_dtype, binary_output, **kwargs)\u001b[0m\n\u001b[0;32m    919\u001b[0m \u001b[38;5;66;03m# We shouldn't call `model.to()` for models loaded with accelerate as well as the case that model is already on device\u001b[39;00m\n\u001b[0;32m    920\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    921\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    922\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m    923\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    924\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m hf_device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    925\u001b[0m ):\n\u001b[1;32m--> 926\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    928\u001b[0m \u001b[38;5;66;03m# If the model can generate, create a local generation config. This is done to avoid side-effects on the model\u001b[39;00m\n\u001b[0;32m    929\u001b[0m \u001b[38;5;66;03m# as we apply local tweaks to the generation config.\u001b[39;00m\n\u001b[0;32m    930\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mcan_generate():\n",
            "File \u001b[1;32md:\\program_files\\anaconda3\\envs\\torch_gpu\\Lib\\site-packages\\transformers\\modeling_utils.py:3164\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3159\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[0;32m   3160\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3161\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3162\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3163\u001b[0m         )\n\u001b[1;32m-> 3164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32md:\\program_files\\anaconda3\\envs\\torch_gpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1340\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1337\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1338\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m-> 1340\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32md:\\program_files\\anaconda3\\envs\\torch_gpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
            "File \u001b[1;32md:\\program_files\\anaconda3\\envs\\torch_gpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
            "File \u001b[1;32md:\\program_files\\anaconda3\\envs\\torch_gpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:927\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    923\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    924\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    925\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    926\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 927\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    928\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    930\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
            "File \u001b[1;32md:\\program_files\\anaconda3\\envs\\torch_gpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1326\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m   1320\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[0;32m   1321\u001b[0m             device,\n\u001b[0;32m   1322\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1323\u001b[0m             non_blocking,\n\u001b[0;32m   1324\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[0;32m   1325\u001b[0m         )\n\u001b[1;32m-> 1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1331\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
            "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ],
      "source": [
        "# Test summarize_text\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "for article in articeles:\n",
        "    print(summarize_text(article[\"full_content\"], summarizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TSQw6Adtmlwo"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "\n",
        "subprocess.run([\"streamlit\", \"run\", \"StreamlitUI.py\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'exceptions'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;167;01mPendingDeprecationWarning\u001b[39;00m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'exceptions'"
          ]
        }
      ],
      "source": [
        "from exceptions import PendingDeprecationWarning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "torch_gpu",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
