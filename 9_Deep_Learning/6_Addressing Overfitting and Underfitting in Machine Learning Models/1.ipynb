{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import datasets\n",
    "import tensorflow as tf\n",
    "import optuna\n",
    "\n",
    "# loading CIFAR100 dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = datasets.cifar100.load_data()\n",
    "def train_augment(image, label):\n",
    "    # normalize image\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_flip_up_down(image)\n",
    "    image = tf.image.rot90(image, k=tf.random.uniform(shape=[], minval=0, maxval=4, dtype=tf.int32))\n",
    "    image = tf.image.random_crop(image, size=[32, 32, 3])\n",
    "    \n",
    "    return image, label  # Return both image and label\n",
    "\n",
    "def test_augment(image, label):\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# splitting validation set and test set\n",
    "test_images, val_images, test_labels, val_labels = train_test_split(test_images, test_labels, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = (\n",
    "    tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
    "    .map(train_augment, num_parallel_calls=tf.data.AUTOTUNE)  # Now works correctly\n",
    "    .batch(32)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "val_ds = (\n",
    "    tf.data.Dataset.from_tensor_slices((val_images, val_labels))\n",
    "    .map(test_augment, num_parallel_calls=tf.data.AUTOTUNE)  # Ensure label is passed correctly\n",
    "    .batch(32)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "test_ds = (\n",
    "    tf.data.Dataset.from_tensor_slices((test_images, test_labels))\n",
    "    .map(test_augment, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .batch(32)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding best architecture complexity between 1, 2 and 3 conv layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(k, filter_size = (3, 3), dropout=0, regularization=0):\n",
    "    block = [\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(k, filter_size, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(regularization)),\n",
    "    ]\n",
    "    if dropout:\n",
    "        block.append(tf.keras.layers.Dropout(dropout))\n",
    "    return block\n",
    "\n",
    "def final_block(dropout = 0, regularization=0):\n",
    "    block = [\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(regularization)),\n",
    "    ]\n",
    "    if dropout:\n",
    "        block.append(tf.keras.layers.Dropout(dropout))\n",
    "    block.append(tf.keras.layers.Dense(100, activation='softmax'))\n",
    "    return block\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(num_layers,base = 5, kernels = (3, 3), regularization=0, dropout_conv=0, dropout_dense=0):\n",
    "    inputs = tf.keras.layers.Input(shape=(32, 32, 3))\n",
    "    x = tf.keras.layers.Conv2D(2**base, kernels, activation='relu')(inputs)\n",
    "    \n",
    "    for layer_number in range(num_layers):\n",
    "        for sub_layer in conv_block(2**(base+layer_number), kernels, dropout_conv, regularization):\n",
    "            try :\n",
    "                x = sub_layer(x)\n",
    "            except ValueError:\n",
    "                pass\n",
    "            \n",
    "    for sub_layer in final_block(dropout_dense,regularization):\n",
    "        x = sub_layer(x)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=x)\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(3, kernels = (7,7), regularization=1, dropout_conv=1, dropout_dense=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    lr = trial.suggest_loguniform('learning_rate', 1e-5, 1e-3)\n",
    "    model = create_model(num_layers = trial.suggest_int('num_layers', 0, 3), \n",
    "                         base = trial.suggest_int('base', 4, 7),\n",
    "                         kernels = trial.suggest_categorical(\"kernels\", [(3, 3), (5, 5), (7, 7)]),\n",
    "                         regularization = trial.suggest_float('regularization', 1e-6, 1e-3, log=True),\n",
    "                         dropout_conv = trial.suggest_float('dropout_conv', 0, 0.3),\n",
    "                         dropout_dense = trial.suggest_float('dropout_dense', 0, 0.3),\n",
    "                         )\n",
    "                         \n",
    "                    \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(train_ds, epochs=10, validation_data=(val_ds), verbose=0)\n",
    "    return model.evaluate(test_ds, verbose=0)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-12 12:27:07,458] A new study created in memory with name: no-name-7254ab8d-18bf-4a4e-9174-a6c5d733cf14\n",
      "[I 2025-02-12 12:30:34,111] Trial 0 finished with value: 0.06639999896287918 and parameters: {'learning_rate': 1.5095468077916711e-05, 'num_layers': 3, 'base': 6, 'kernels': (7, 7), 'regularization': 0.0001979669373286028, 'dropout_conv': 0.2923313796841476, 'dropout_dense': 0.12002978223693585}. Best is trial 0 with value: 0.06639999896287918.\n",
      "[I 2025-02-12 12:32:53,007] Trial 1 finished with value: 0.06419999897480011 and parameters: {'learning_rate': 0.00011472347754379394, 'num_layers': 1, 'base': 7, 'kernels': (3, 3), 'regularization': 3.198444845007553e-05, 'dropout_conv': 0.05891105885245885, 'dropout_dense': 0.17764114906686554}. Best is trial 0 with value: 0.06639999896287918.\n",
      "[I 2025-02-12 12:56:29,270] Trial 2 finished with value: 0.05040000006556511 and parameters: {'learning_rate': 0.0005482136313599965, 'num_layers': 1, 'base': 6, 'kernels': (3, 3), 'regularization': 6.513364350916589e-05, 'dropout_conv': 0.13658980270166712, 'dropout_dense': 0.276345012802523}. Best is trial 0 with value: 0.06639999896287918.\n",
      "[I 2025-02-12 12:57:32,221] Trial 3 finished with value: 0.07900000363588333 and parameters: {'learning_rate': 0.00013317542794255238, 'num_layers': 0, 'base': 5, 'kernels': (5, 5), 'regularization': 0.0001890739349264509, 'dropout_conv': 0.11031456189420243, 'dropout_dense': 0.26131358796128096}. Best is trial 3 with value: 0.07900000363588333.\n",
      "[I 2025-02-12 13:00:05,655] Trial 4 finished with value: 0.07119999825954437 and parameters: {'learning_rate': 2.4226026083736792e-05, 'num_layers': 2, 'base': 7, 'kernels': (5, 5), 'regularization': 2.226552139105885e-06, 'dropout_conv': 0.2895220147947962, 'dropout_dense': 0.2432199344753104}. Best is trial 3 with value: 0.07900000363588333.\n",
      "[I 2025-02-12 13:01:06,208] Trial 5 finished with value: 0.07859999686479568 and parameters: {'learning_rate': 4.903248149792175e-05, 'num_layers': 0, 'base': 4, 'kernels': (7, 7), 'regularization': 0.00014090909512390885, 'dropout_conv': 0.04957449550848174, 'dropout_dense': 0.22900882218997723}. Best is trial 3 with value: 0.07900000363588333.\n",
      "[I 2025-02-12 13:02:10,417] Trial 6 finished with value: 0.07100000232458115 and parameters: {'learning_rate': 6.41957363800424e-05, 'num_layers': 0, 'base': 5, 'kernels': (5, 5), 'regularization': 0.00024494412431519744, 'dropout_conv': 0.09841303542286421, 'dropout_dense': 0.080183680579885}. Best is trial 3 with value: 0.07900000363588333.\n",
      "[I 2025-02-12 13:03:39,060] Trial 7 finished with value: 0.06639999896287918 and parameters: {'learning_rate': 0.0002338465262146101, 'num_layers': 3, 'base': 5, 'kernels': (3, 3), 'regularization': 5.3998852609072306e-05, 'dropout_conv': 0.24535652109088726, 'dropout_dense': 0.1176542856608367}. Best is trial 3 with value: 0.07900000363588333.\n",
      "[I 2025-02-12 13:05:11,075] Trial 8 finished with value: 0.051600001752376556 and parameters: {'learning_rate': 0.0001323219965925716, 'num_layers': 3, 'base': 6, 'kernels': (3, 3), 'regularization': 1.986467260710151e-05, 'dropout_conv': 0.12469843616501051, 'dropout_dense': 0.006585309436066278}. Best is trial 3 with value: 0.07900000363588333.\n",
      "[I 2025-02-12 13:06:25,877] Trial 9 finished with value: 0.07479999959468842 and parameters: {'learning_rate': 0.00010351034414800285, 'num_layers': 1, 'base': 6, 'kernels': (3, 3), 'regularization': 1.246942925208816e-05, 'dropout_conv': 0.08320076469363874, 'dropout_dense': 0.09232683446504236}. Best is trial 3 with value: 0.07900000363588333.\n",
      "[I 2025-02-12 13:07:27,536] Trial 10 finished with value: 0.0934000015258789 and parameters: {'learning_rate': 0.0007880106925432242, 'num_layers': 0, 'base': 4, 'kernels': (5, 5), 'regularization': 0.0007639277527818383, 'dropout_conv': 2.532795440943203e-05, 'dropout_dense': 0.2961538059712999}. Best is trial 10 with value: 0.0934000015258789.\n",
      "[I 2025-02-12 13:08:29,240] Trial 11 finished with value: 0.061799999326467514 and parameters: {'learning_rate': 0.0009216625039983999, 'num_layers': 0, 'base': 4, 'kernels': (5, 5), 'regularization': 0.0008828971327498394, 'dropout_conv': 0.005621454582524638, 'dropout_dense': 0.29905154250856997}. Best is trial 10 with value: 0.0934000015258789.\n",
      "[I 2025-02-12 13:09:36,206] Trial 12 finished with value: 0.04600000008940697 and parameters: {'learning_rate': 0.0003123466304194047, 'num_layers': 0, 'base': 4, 'kernels': (5, 5), 'regularization': 0.0008851418586778068, 'dropout_conv': 0.20155070292631544, 'dropout_dense': 0.21460296664797932}. Best is trial 10 with value: 0.0934000015258789.\n",
      "[I 2025-02-12 13:11:07,171] Trial 13 finished with value: 0.058400001376867294 and parameters: {'learning_rate': 0.0008726044764313521, 'num_layers': 2, 'base': 5, 'kernels': (5, 5), 'regularization': 0.0003319474138439816, 'dropout_conv': 0.001302404235027893, 'dropout_dense': 0.26854004316139424}. Best is trial 10 with value: 0.0934000015258789.\n",
      "[I 2025-02-12 13:12:27,574] Trial 14 finished with value: 0.0706000030040741 and parameters: {'learning_rate': 0.0003056075175447611, 'num_layers': 1, 'base': 4, 'kernels': (5, 5), 'regularization': 0.0005228003467811258, 'dropout_conv': 0.17850680340371447, 'dropout_dense': 0.1813489799572362}. Best is trial 10 with value: 0.0934000015258789.\n",
      "[I 2025-02-12 13:13:50,486] Trial 15 finished with value: 0.07479999959468842 and parameters: {'learning_rate': 3.091817005836464e-05, 'num_layers': 0, 'base': 5, 'kernels': (5, 5), 'regularization': 0.00010728890008616652, 'dropout_conv': 0.03832775971479836, 'dropout_dense': 0.29313113674500485}. Best is trial 10 with value: 0.0934000015258789.\n",
      "[I 2025-02-12 13:15:03,548] Trial 16 finished with value: 0.0737999975681305 and parameters: {'learning_rate': 0.0002006005376415868, 'num_layers': 0, 'base': 4, 'kernels': (7, 7), 'regularization': 7.116523826370857e-06, 'dropout_conv': 0.18645200762583436, 'dropout_dense': 0.19310509349280988}. Best is trial 10 with value: 0.0934000015258789.\n",
      "[I 2025-02-12 13:16:35,631] Trial 17 finished with value: 0.07240000367164612 and parameters: {'learning_rate': 0.00045712495054062705, 'num_layers': 2, 'base': 5, 'kernels': (5, 5), 'regularization': 0.0004616100458483145, 'dropout_conv': 0.09218193742885822, 'dropout_dense': 0.2554419566688627}. Best is trial 10 with value: 0.0934000015258789.\n",
      "[I 2025-02-12 13:17:56,023] Trial 18 finished with value: 0.06539999693632126 and parameters: {'learning_rate': 1.130260543659336e-05, 'num_layers': 1, 'base': 4, 'kernels': (5, 5), 'regularization': 1.0047956899937575e-06, 'dropout_conv': 0.2316203474702347, 'dropout_dense': 0.22249106997133877}. Best is trial 10 with value: 0.0934000015258789.\n",
      "[I 2025-02-12 13:19:01,301] Trial 19 finished with value: 0.06159999966621399 and parameters: {'learning_rate': 6.136872817025139e-05, 'num_layers': 0, 'base': 5, 'kernels': (5, 5), 'regularization': 9.827166817406977e-05, 'dropout_conv': 0.1517959469492946, 'dropout_dense': 0.15328170209237532}. Best is trial 10 with value: 0.0934000015258789.\n",
      "[I 2025-02-12 13:20:17,325] Trial 20 finished with value: 0.055799998342990875 and parameters: {'learning_rate': 0.0005364851761968429, 'num_layers': 1, 'base': 4, 'kernels': (7, 7), 'regularization': 0.0005117803906425768, 'dropout_conv': 0.03080363672707244, 'dropout_dense': 0.2631400499290407}. Best is trial 10 with value: 0.0934000015258789.\n",
      "[I 2025-02-12 13:21:21,617] Trial 21 finished with value: 0.1080000028014183 and parameters: {'learning_rate': 4.731127278325567e-05, 'num_layers': 0, 'base': 4, 'kernels': (7, 7), 'regularization': 0.0001805946001496727, 'dropout_conv': 0.0591442568037967, 'dropout_dense': 0.23107856984665898}. Best is trial 21 with value: 0.1080000028014183.\n",
      "[I 2025-02-12 13:22:24,859] Trial 22 finished with value: 0.07900000363588333 and parameters: {'learning_rate': 3.40453661396262e-05, 'num_layers': 0, 'base': 4, 'kernels': (7, 7), 'regularization': 0.00023158039161814484, 'dropout_conv': 0.07003371576636327, 'dropout_dense': 0.2988508295211504}. Best is trial 21 with value: 0.1080000028014183.\n",
      "[I 2025-02-12 13:23:28,306] Trial 23 finished with value: 0.09279999881982803 and parameters: {'learning_rate': 0.00017408888023375224, 'num_layers': 0, 'base': 5, 'kernels': (7, 7), 'regularization': 0.0009571107953761079, 'dropout_conv': 0.10919057021939549, 'dropout_dense': 0.24312036906380208}. Best is trial 21 with value: 0.1080000028014183.\n",
      "[I 2025-02-12 13:24:29,984] Trial 24 finished with value: 0.07320000231266022 and parameters: {'learning_rate': 6.814779432731971e-05, 'num_layers': 0, 'base': 4, 'kernels': (7, 7), 'regularization': 0.0008923791861732875, 'dropout_conv': 0.019877351073809838, 'dropout_dense': 0.215817076436711}. Best is trial 21 with value: 0.1080000028014183.\n",
      "[I 2025-02-12 13:25:50,493] Trial 25 finished with value: 0.0812000036239624 and parameters: {'learning_rate': 0.0001705546930568152, 'num_layers': 1, 'base': 5, 'kernels': (7, 7), 'regularization': 0.0005018722195903992, 'dropout_conv': 0.06592848645401257, 'dropout_dense': 0.24361266180574362}. Best is trial 21 with value: 0.1080000028014183.\n",
      "[I 2025-02-12 13:26:53,891] Trial 26 finished with value: 0.09059999883174896 and parameters: {'learning_rate': 4.051861494424919e-05, 'num_layers': 0, 'base': 4, 'kernels': (7, 7), 'regularization': 0.00038051708936601055, 'dropout_conv': 0.030253238939577112, 'dropout_dense': 0.19635231812394005}. Best is trial 21 with value: 0.1080000028014183.\n",
      "[I 2025-02-12 13:28:09,339] Trial 27 finished with value: 0.0658000037074089 and parameters: {'learning_rate': 2.3499311191477917e-05, 'num_layers': 1, 'base': 4, 'kernels': (7, 7), 'regularization': 0.000911655398821789, 'dropout_conv': 0.1559697600469357, 'dropout_dense': 0.27434909065349916}. Best is trial 21 with value: 0.1080000028014183.\n",
      "[I 2025-02-12 13:29:11,040] Trial 28 finished with value: 0.1340000033378601 and parameters: {'learning_rate': 0.00032430897891660293, 'num_layers': 0, 'base': 5, 'kernels': (7, 7), 'regularization': 5.162667587400127e-05, 'dropout_conv': 0.11100109380399757, 'dropout_dense': 0.15221621493494913}. Best is trial 28 with value: 0.1340000033378601.\n",
      "[I 2025-02-12 13:30:39,242] Trial 29 finished with value: 0.08799999952316284 and parameters: {'learning_rate': 0.0007752746877582322, 'num_layers': 2, 'base': 6, 'kernels': (7, 7), 'regularization': 4.687765446372846e-05, 'dropout_conv': 0.0726338264889788, 'dropout_dense': 0.14246958324836823}. Best is trial 28 with value: 0.1340000033378601.\n",
      "[I 2025-02-12 13:32:24,585] Trial 30 finished with value: 0.11240000277757645 and parameters: {'learning_rate': 0.00037214155362502087, 'num_layers': 0, 'base': 7, 'kernels': (7, 7), 'regularization': 8.464723698471903e-05, 'dropout_conv': 0.043784276233223896, 'dropout_dense': 0.04115556703369809}. Best is trial 28 with value: 0.1340000033378601.\n",
      "[I 2025-02-12 13:33:41,983] Trial 31 finished with value: 0.0820000022649765 and parameters: {'learning_rate': 0.000389811379077659, 'num_layers': 0, 'base': 6, 'kernels': (7, 7), 'regularization': 9.944739885021717e-05, 'dropout_conv': 0.04646273765213923, 'dropout_dense': 0.022352196628084266}. Best is trial 28 with value: 0.1340000033378601.\n",
      "[I 2025-02-12 13:35:25,717] Trial 32 finished with value: 0.0835999995470047 and parameters: {'learning_rate': 0.0006476804470384172, 'num_layers': 0, 'base': 7, 'kernels': (7, 7), 'regularization': 2.1556333676309915e-05, 'dropout_conv': 0.016488949787976402, 'dropout_dense': 0.07878708377982105}. Best is trial 28 with value: 0.1340000033378601.\n",
      "[I 2025-02-12 13:37:45,955] Trial 33 finished with value: 0.11299999803304672 and parameters: {'learning_rate': 0.00032961573537716737, 'num_layers': 1, 'base': 7, 'kernels': (7, 7), 'regularization': 3.139627094474365e-05, 'dropout_conv': 0.0562787017467155, 'dropout_dense': 0.049551333399459344}. Best is trial 28 with value: 0.1340000033378601.\n",
      "[I 2025-02-12 13:40:03,515] Trial 34 finished with value: 0.10700000077486038 and parameters: {'learning_rate': 0.0002717093650929222, 'num_layers': 1, 'base': 7, 'kernels': (7, 7), 'regularization': 3.3586567701792815e-05, 'dropout_conv': 0.05528201381112305, 'dropout_dense': 0.045033090643070414}. Best is trial 28 with value: 0.1340000033378601.\n",
      "[I 2025-02-12 13:42:20,021] Trial 35 finished with value: 0.0828000009059906 and parameters: {'learning_rate': 8.705245307665236e-05, 'num_layers': 1, 'base': 7, 'kernels': (7, 7), 'regularization': 6.670880415971744e-05, 'dropout_conv': 0.112755012460699, 'dropout_dense': 0.059581027786885524}. Best is trial 28 with value: 0.1340000033378601.\n",
      "[I 2025-02-12 13:44:36,528] Trial 36 finished with value: 0.11699999868869781 and parameters: {'learning_rate': 0.0004218887815704549, 'num_layers': 1, 'base': 7, 'kernels': (7, 7), 'regularization': 9.292626132814488e-06, 'dropout_conv': 0.08260238894002939, 'dropout_dense': 0.03627459333602687}. Best is trial 28 with value: 0.1340000033378601.\n",
      "[I 2025-02-12 13:46:55,979] Trial 37 finished with value: 0.1143999993801117 and parameters: {'learning_rate': 0.00037835902198966214, 'num_layers': 2, 'base': 7, 'kernels': (7, 7), 'regularization': 7.089552322300716e-06, 'dropout_conv': 0.1353935296836236, 'dropout_dense': 0.03925060287247707}. Best is trial 28 with value: 0.1340000033378601.\n",
      "[I 2025-02-12 13:49:15,690] Trial 38 finished with value: 0.09740000218153 and parameters: {'learning_rate': 0.0005596155129082372, 'num_layers': 2, 'base': 7, 'kernels': (7, 7), 'regularization': 5.2213354934657345e-06, 'dropout_conv': 0.13802771010341933, 'dropout_dense': 0.10903870265818721}. Best is trial 28 with value: 0.1340000033378601.\n",
      "[I 2025-02-12 13:51:09,594] Trial 39 finished with value: 0.09619999676942825 and parameters: {'learning_rate': 0.00037170412173512127, 'num_layers': 2, 'base': 7, 'kernels': (3, 3), 'regularization': 5.017978884174095e-06, 'dropout_conv': 0.12735289253302268, 'dropout_dense': 0.010902753935652335}. Best is trial 28 with value: 0.1340000033378601.\n",
      "[I 2025-02-12 13:53:30,289] Trial 40 finished with value: 0.07840000092983246 and parameters: {'learning_rate': 0.00022697043008504668, 'num_layers': 3, 'base': 7, 'kernels': (7, 7), 'regularization': 1.0428538247632822e-05, 'dropout_conv': 0.16535587450555092, 'dropout_dense': 0.14713545535898034}. Best is trial 28 with value: 0.1340000033378601.\n",
      "[I 2025-02-12 13:55:49,431] Trial 41 finished with value: 0.10819999873638153 and parameters: {'learning_rate': 0.0004118646320121527, 'num_layers': 2, 'base': 7, 'kernels': (7, 7), 'regularization': 2.1004779588563373e-05, 'dropout_conv': 0.08713932278868824, 'dropout_dense': 0.03724658071031619}. Best is trial 28 with value: 0.1340000033378601.\n",
      "[I 2025-02-12 13:58:06,713] Trial 42 finished with value: 0.10400000214576721 and parameters: {'learning_rate': 0.0003335242026408471, 'num_layers': 1, 'base': 7, 'kernels': (7, 7), 'regularization': 2.8421844256253456e-06, 'dropout_conv': 0.10410074795553759, 'dropout_dense': 0.06134812651366339}. Best is trial 28 with value: 0.1340000033378601.\n",
      "[I 2025-02-12 13:59:44,610] Trial 43 finished with value: 0.08320000022649765 and parameters: {'learning_rate': 0.0002601076344672411, 'num_layers': 2, 'base': 6, 'kernels': (7, 7), 'regularization': 1.1688421634389096e-05, 'dropout_conv': 0.0774714024832586, 'dropout_dense': 0.03160861772755858}. Best is trial 28 with value: 0.1340000033378601.\n",
      "[I 2025-02-12 14:01:36,844] Trial 44 finished with value: 0.0722000002861023 and parameters: {'learning_rate': 0.0005972453759192043, 'num_layers': 1, 'base': 7, 'kernels': (3, 3), 'regularization': 3.656957623611907e-05, 'dropout_conv': 0.14216397979701415, 'dropout_dense': 0.05460919332754003}. Best is trial 28 with value: 0.1340000033378601.\n",
      "[I 2025-02-12 14:03:08,733] Trial 45 finished with value: 0.11420000344514847 and parameters: {'learning_rate': 0.0004580442240559818, 'num_layers': 1, 'base': 6, 'kernels': (7, 7), 'regularization': 1.7350483915376667e-05, 'dropout_conv': 0.11833829841718355, 'dropout_dense': 0.0016920667438160633}. Best is trial 28 with value: 0.1340000033378601.\n",
      "[I 2025-02-12 14:04:46,162] Trial 46 finished with value: 0.08619999885559082 and parameters: {'learning_rate': 0.00048605657545822763, 'num_layers': 1, 'base': 6, 'kernels': (7, 7), 'regularization': 1.658141708188278e-05, 'dropout_conv': 0.12225166681739397, 'dropout_dense': 0.00748028969773477}. Best is trial 28 with value: 0.1340000033378601.\n",
      "[I 2025-02-12 14:06:22,567] Trial 47 finished with value: 0.09019999951124191 and parameters: {'learning_rate': 0.00014206290409771416, 'num_layers': 1, 'base': 6, 'kernels': (7, 7), 'regularization': 7.760531737526798e-06, 'dropout_conv': 0.12297021738258573, 'dropout_dense': 0.07526588874557358}. Best is trial 28 with value: 0.1340000033378601.\n",
      "[I 2025-02-12 14:08:04,339] Trial 48 finished with value: 0.07519999891519547 and parameters: {'learning_rate': 0.0007016127481387709, 'num_layers': 3, 'base': 6, 'kernels': (3, 3), 'regularization': 1.4854223683577015e-05, 'dropout_conv': 0.08908462449210358, 'dropout_dense': 0.10366609309559821}. Best is trial 28 with value: 0.1340000033378601.\n",
      "[I 2025-02-12 14:10:34,923] Trial 49 finished with value: 0.08100000023841858 and parameters: {'learning_rate': 0.00021402654746477485, 'num_layers': 2, 'base': 7, 'kernels': (7, 7), 'regularization': 2.6906138858938137e-06, 'dropout_conv': 0.1649500055715612, 'dropout_dense': 0.020706750959105853}. Best is trial 28 with value: 0.1340000033378601.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'learning_rate': 0.00032430897891660293, 'num_layers': 0, 'base': 5, 'kernels': (7, 7), 'regularization': 5.162667587400127e-05, 'dropout_conv': 0.11100109380399757, 'dropout_dense': 0.15221621493494913}\n"
     ]
    }
   ],
   "source": [
    "# Run Optuna optimization\n",
    "study = optuna.create_study(direction=\"maximize\")  \n",
    "study.optimize(objective, n_trials=50)\n",
    "# Best hyperparameters\n",
    "print(\"Best hyperparameters:\", study.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optuna optimizes hyperparameters using search algorithms, such as:\n",
    "\n",
    "1️⃣ Bayesian Optimization (TPE - Tree-structured Parzen Estimator)\n",
    "- Learns from past trials to suggest better hyperparameters.\n",
    "- Efficient for complex search spaces.\n",
    "\n",
    "2️⃣ Grid Search & Random Search\n",
    "- Random search: Tries random values in the defined range.\n",
    "- Grid search: Tests all possible combinations (not recommended for large search spaces).\n",
    "\n",
    "3️⃣ Pruning (Early Stopping)\n",
    "- Stops bad trials early to save compute time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the model\n",
    "with open('best_params.pkl', 'wb') as f:\n",
    "    pickle.dump(study.best_params, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(study.trials_dataframe()).to_csv('study.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>learning_rate</th>\n",
       "      <td>0.000324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_layers</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>base</th>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kernels</th>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>regularization</th>\n",
       "      <td>0.000052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dropout_conv</th>\n",
       "      <td>0.111001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dropout_dense</th>\n",
       "      <td>0.152216</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       0\n",
       "learning_rate   0.000324\n",
       "num_layers      0.000000\n",
       "base            5.000000\n",
       "kernels         7.000000\n",
       "regularization  0.000052\n",
       "dropout_conv    0.111001\n",
       "dropout_dense   0.152216"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(study.best_params).iloc[:1].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = study.best_params.copy()\n",
    "del best_params['learning_rate']\n",
    "model = create_model(**best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmlahmadmostafa\u001b[0m (\u001b[33mmlahmadmostafa-\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\Industry\\Projects\\Sprints - Ai\\12_DL\\Addressing Overfitting and Underfitting in Machine Learning Models\\wandb\\run-20250212_141058-jhn59fgs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mlahmadmostafa-/cifar100/runs/jhn59fgs' target=\"_blank\">devout-smoke-3</a></strong> to <a href='https://wandb.ai/mlahmadmostafa-/cifar100' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mlahmadmostafa-/cifar100' target=\"_blank\">https://wandb.ai/mlahmadmostafa-/cifar100</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mlahmadmostafa-/cifar100/runs/jhn59fgs' target=\"_blank\">https://wandb.ai/mlahmadmostafa-/cifar100/runs/jhn59fgs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# WANDB\n",
    "import wandb\n",
    "from wandb.integration.keras import WandbMetricsLogger\n",
    "\n",
    "# Start a new W&B run\n",
    "wandb.init(project=\"cifar100\")\n",
    "\n",
    "# learning rate reducer\n",
    "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\", factor=0.5, patience=3, min_lr=1e-6\n",
    ")\n",
    "# Early stopping for f1\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class F1Score(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name='f1_score', **kwargs):\n",
    "        super(F1Score, self).__init__(name=name, **kwargs)\n",
    "        self.true_positives = self.add_weight(name='tp', initializer='zeros')\n",
    "        self.false_positives = self.add_weight(name='fp', initializer='zeros')\n",
    "        self.false_negatives = self.add_weight(name='fn', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred = tf.round(tf.clip_by_value(y_pred, 0, 1))\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "\n",
    "        tp = tf.reduce_sum(tf.cast(y_true * y_pred, tf.float32))\n",
    "        fp = tf.reduce_sum(tf.cast((1 - y_true) * y_pred, tf.float32))\n",
    "        fn = tf.reduce_sum(tf.cast(y_true * (1 - y_pred), tf.float32))\n",
    "\n",
    "        self.true_positives.assign_add(tp)\n",
    "        self.false_positives.assign_add(fp)\n",
    "        self.false_negatives.assign_add(fn)\n",
    "\n",
    "    def result(self):\n",
    "        precision = self.true_positives / (self.true_positives + self.false_positives + tf.keras.backend.epsilon())\n",
    "        recall = self.true_positives / (self.true_positives + self.false_negatives + tf.keras.backend.epsilon())\n",
    "        return 2 * ((precision * recall) / (precision + recall + tf.keras.backend.epsilon()))\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.true_positives.assign(0)\n",
    "        self.false_positives.assign(0)\n",
    "        self.false_negatives.assign(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "   5/1563 [..............................] - ETA: 42s - loss: 4.6415 - accuracy: 0.0125 - f1_score: 0.0000e+00    WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0101s vs `on_train_batch_end` time: 0.0144s). Check your callbacks.\n",
      "1563/1563 [==============================] - 11s 6ms/step - loss: 4.3399 - accuracy: 0.0457 - f1_score: 8.3459e-05 - val_loss: 270.4903 - val_accuracy: 0.0664 - val_f1_score: 0.0200 - lr: 3.2431e-04\n",
      "Epoch 2/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 4.0075 - accuracy: 0.0875 - f1_score: 2.6628e-04 - val_loss: 308.2172 - val_accuracy: 0.0854 - val_f1_score: 0.0200 - lr: 3.2431e-04\n",
      "Epoch 3/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 3.8494 - accuracy: 0.1109 - f1_score: 3.7922e-04 - val_loss: 396.6303 - val_accuracy: 0.0912 - val_f1_score: 0.0200 - lr: 3.2431e-04\n",
      "Epoch 4/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 3.7542 - accuracy: 0.1246 - f1_score: 4.6377e-04 - val_loss: 413.4002 - val_accuracy: 0.0916 - val_f1_score: 0.0200 - lr: 3.2431e-04\n",
      "Epoch 5/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 3.6492 - accuracy: 0.1427 - f1_score: 5.4109e-04 - val_loss: 417.9102 - val_accuracy: 0.0928 - val_f1_score: 0.0200 - lr: 1.6215e-04\n",
      "Epoch 6/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 3.5953 - accuracy: 0.1537 - f1_score: 5.7201e-04 - val_loss: 459.1493 - val_accuracy: 0.0874 - val_f1_score: 0.0200 - lr: 1.6215e-04\n",
      "Epoch 7/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 3.5633 - accuracy: 0.1588 - f1_score: 6.5607e-04 - val_loss: 456.4174 - val_accuracy: 0.0856 - val_f1_score: 0.0200 - lr: 1.6215e-04\n",
      "Epoch 8/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 3.5091 - accuracy: 0.1670 - f1_score: 7.2389e-04 - val_loss: 439.5421 - val_accuracy: 0.0966 - val_f1_score: 0.0200 - lr: 8.1077e-05\n",
      "Epoch 9/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 3.4921 - accuracy: 0.1707 - f1_score: 7.2611e-04 - val_loss: 464.4938 - val_accuracy: 0.0926 - val_f1_score: 0.0200 - lr: 8.1077e-05\n",
      "Epoch 10/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 3.4701 - accuracy: 0.1749 - f1_score: 7.8938e-04 - val_loss: 472.0099 - val_accuracy: 0.0954 - val_f1_score: 0.0200 - lr: 8.1077e-05\n",
      "Epoch 11/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 3.4428 - accuracy: 0.1783 - f1_score: 8.1039e-04 - val_loss: 450.6837 - val_accuracy: 0.0968 - val_f1_score: 0.0200 - lr: 4.0539e-05\n",
      "Epoch 12/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 3.4348 - accuracy: 0.1786 - f1_score: 8.1457e-04 - val_loss: 461.1006 - val_accuracy: 0.0960 - val_f1_score: 0.0200 - lr: 4.0539e-05\n",
      "Epoch 13/100\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 3.4243 - accuracy: 0.1818 - f1_score: 8.5415e-04 - val_loss: 452.5962 - val_accuracy: 0.0954 - val_f1_score: 0.0200 - lr: 4.0539e-05\n",
      "Epoch 14/100\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 3.4151 - accuracy: 0.1813 - f1_score: 8.4257e-04 - val_loss: 457.8607 - val_accuracy: 0.0950 - val_f1_score: 0.0200 - lr: 2.0269e-05\n",
      "Epoch 15/100\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 3.4051 - accuracy: 0.1836 - f1_score: 8.6459e-04 - val_loss: 456.7755 - val_accuracy: 0.0960 - val_f1_score: 0.0200 - lr: 2.0269e-05\n",
      "Epoch 16/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 3.4053 - accuracy: 0.1847 - f1_score: 8.6190e-04 - val_loss: 454.8077 - val_accuracy: 0.0972 - val_f1_score: 0.0200 - lr: 2.0269e-05\n",
      "Epoch 17/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 3.3948 - accuracy: 0.1867 - f1_score: 8.5458e-04 - val_loss: 452.4669 - val_accuracy: 0.0946 - val_f1_score: 0.0200 - lr: 1.0135e-05\n",
      "Epoch 18/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 3.3974 - accuracy: 0.1847 - f1_score: 8.3864e-04 - val_loss: 446.7433 - val_accuracy: 0.0960 - val_f1_score: 0.0200 - lr: 1.0135e-05\n",
      "Epoch 19/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 3.3911 - accuracy: 0.1843 - f1_score: 9.0142e-04 - val_loss: 454.7448 - val_accuracy: 0.0948 - val_f1_score: 0.0200 - lr: 1.0135e-05\n",
      "Epoch 20/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 3.3915 - accuracy: 0.1862 - f1_score: 8.6778e-04 - val_loss: 453.3176 - val_accuracy: 0.0950 - val_f1_score: 0.0200 - lr: 5.0673e-06\n",
      "Epoch 21/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 3.3913 - accuracy: 0.1860 - f1_score: 8.7897e-04 - val_loss: 455.2397 - val_accuracy: 0.0948 - val_f1_score: 0.0200 - lr: 5.0673e-06\n",
      "Epoch 22/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 3.3857 - accuracy: 0.1879 - f1_score: 8.7215e-04 - val_loss: 457.4930 - val_accuracy: 0.0950 - val_f1_score: 0.0200 - lr: 5.0673e-06\n",
      "Epoch 23/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 3.3844 - accuracy: 0.1880 - f1_score: 9.2326e-04 - val_loss: 451.9007 - val_accuracy: 0.0954 - val_f1_score: 0.0200 - lr: 2.5337e-06\n",
      "Epoch 24/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 3.3843 - accuracy: 0.1875 - f1_score: 8.9001e-04 - val_loss: 454.8086 - val_accuracy: 0.0950 - val_f1_score: 0.0200 - lr: 2.5337e-06\n",
      "Epoch 25/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 3.3874 - accuracy: 0.1870 - f1_score: 8.6161e-04 - val_loss: 457.0296 - val_accuracy: 0.0944 - val_f1_score: 0.0200 - lr: 2.5337e-06\n",
      "Epoch 26/100\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 3.3871 - accuracy: 0.1861 - f1_score: 8.9210e-04 - val_loss: 454.2098 - val_accuracy: 0.0940 - val_f1_score: 0.0200 - lr: 1.2668e-06\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2150e5d8280>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=study.best_params['learning_rate']), \n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy', F1Score()])\n",
    "\n",
    "model.fit(train_ds, epochs=100, validation_data=(val_ds), \n",
    "          callbacks=[WandbMetricsLogger(), early_stopping, lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
