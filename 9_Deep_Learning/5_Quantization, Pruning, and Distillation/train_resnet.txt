
def train(model, dataloader, device, ):
    for batch_idx, (data, target) in enumerate(dataloader):
        model.train()
        data, target = data.to(device), target.to(device)
        optimizer.zero_grad()
        output = model(data)
        
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()
        
        if batch_idx % 10 == 0:
            print(f"\rBatch: { batch_idx}/{len(dataloader)}, Loss: {loss.item():.4f}                           ", end = '')
    train_loss, train_acc = test(model, dataloader, device)
    return train_loss, train_acc

def test(model, dataloader, device):
    model.eval()
    test_loss = 0
    correct = 0
    with torch.no_grad():
        for data, target in dataloader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            
            test_loss += criterion(output, target).item()  # sum up batch loss

            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability
            correct += pred.eq(target.view_as(pred)).sum().item()

    test_loss /= len(dataloader.dataset)

    return test_loss, 100. * correct / len(dataloader.dataset)
    # # Training 
    # training_loss = []
    # training_accuracy = []

    # for epoch in range(100):
    #     print("Epoch", epoch+1)
    #     train_loss, train_acc = train(model, train_dataloader, device)
    #     test_loss, test_acc = test(model, test_dataloader, device)
    #     print(f"\nTrain Loss = {train_loss:.4f}, Train Accuracy = {train_acc:.2f}")
    #     training_loss.append(test_loss)
    #     training_accuracy.append(test_acc)
    #     print(f"Test Loss = {test_loss:.4f}, Test Accuracy = {test_acc:.2f}")